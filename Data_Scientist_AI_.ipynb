{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f22c8b5-21e1-4d06-8f67-c3f76c3e5175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app5.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from fpdf import FPDF\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Time Series Model Imports ---\n",
    "# Note: You may need to install these libraries: pip install statsmodels prophet fpdf\n",
    "try:\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    from prophet import Prophet\n",
    "    from prophet.plot import plot_plotly, plot_components_plotly\n",
    "    from prophet.plot import plot_components as prophet_plot_components\n",
    "    PROPHET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PROPHET_AVAILABLE = False\n",
    "\n",
    "# --- Advanced Model Imports ---\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGBM_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "\n",
    "\n",
    "# --- Page Configuration ---\n",
    "st.set_page_config(\n",
    "    page_title=\"AI Data Science Pipeline\",\n",
    "    page_icon=\"ï¿½\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# --- Custom PDF Class with Footer ---\n",
    "class PDF(FPDF):\n",
    "    def footer(self):\n",
    "        self.set_y(-15)\n",
    "        self.set_font('Arial', 'I', 8)\n",
    "        # Page number\n",
    "        self.cell(0, 10, 'Page ' + str(self.page_no()) + '/{nb}', 0, 0, 'C')\n",
    "        # Generation date\n",
    "        self.set_x(10)\n",
    "        self.cell(0, 10, f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", 0, 0, 'L')\n",
    "\n",
    "\n",
    "# --- Custom CSS for Black Theme ---\n",
    "def apply_custom_theme():\n",
    "    \"\"\"Applies a dark, clean theme to the Streamlit app.\"\"\"\n",
    "    st.markdown(\"\"\"\n",
    "    <style>\n",
    "        /* Main background color */\n",
    "        .stApp {\n",
    "            background-color: #0E1117;\n",
    "            color: #FAFAFA;\n",
    "        }\n",
    "\n",
    "        /* Sidebar styling */\n",
    "        .st-emotion-cache-16txtl3 {\n",
    "            background-color: #1F222B;\n",
    "        }\n",
    "        \n",
    "        /* Text color in sidebar */\n",
    "        .st-emotion-cache-16txtl3 h1, .st-emotion-cache-16txtl3 h2, .st-emotion-cache-16txtl3 h3, .st-emotion-cache-16txtl3 .st-emotion-cache-1v0mbdj p {\n",
    "            color: #FAFAFA;\n",
    "        }\n",
    "\n",
    "        /* Button styling */\n",
    "        .stButton>button {\n",
    "            border: 2px solid #8A2BE2; /* Purple */\n",
    "            background-color: #8A2BE2; /* Purple */\n",
    "            color: white;\n",
    "            padding: 10px 24px;\n",
    "            text-align: center;\n",
    "            text-decoration: none;\n",
    "            display: inline-block;\n",
    "            font-size: 16px;\n",
    "            margin: 4px 2px;\n",
    "            cursor: pointer;\n",
    "            border-radius: 0.5rem;\n",
    "            transition-duration: 0.4s;\n",
    "            width: 100%;\n",
    "        }\n",
    "        .stButton>button:hover {\n",
    "            background-color: #6B21A8; /* Darker Purple */\n",
    "            border-color: #6B21A8;\n",
    "        }\n",
    "        \n",
    "        /* Header and subheader styling */\n",
    "        h1, h2, h3 {\n",
    "            color: #8A2BE2; /* Purple accent color */\n",
    "        }\n",
    "        \n",
    "        /* Dataframe styling */\n",
    "        .stDataFrame {\n",
    "            border: 1px solid #8A2BE2; /* Purple */\n",
    "            border-radius: 8px;\n",
    "        }\n",
    "\n",
    "        /* Custom Tab Styling */\n",
    "        button[data-baseweb=\"tab\"] {\n",
    "            font-size: 16px !important;\n",
    "            padding-top: 12px !important;\n",
    "            padding-bottom: 12px !important;\n",
    "        }\n",
    "        button[data-baseweb=\"tab\"] > div {\n",
    "            gap: 8px !important;\n",
    "        }\n",
    "\n",
    "    </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "@st.cache_data\n",
    "def get_column_types(df):\n",
    "    \"\"\"Identifies and categorizes columns in a DataFrame.\"\"\"\n",
    "    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    date_cols = df.select_dtypes(include=['datetime', 'datetimetz']).columns.tolist()\n",
    "    \n",
    "    categorical_cols = [col for col in categorical_cols if col not in date_cols]\n",
    "    \n",
    "    id_cols = []\n",
    "    for col in numerical_cols:\n",
    "        if df[col].nunique() == len(df):\n",
    "            id_cols.append(col)\n",
    "    \n",
    "    numerical_cols = [col for col in numerical_cols if col not in id_cols]\n",
    "    return numerical_cols, categorical_cols, date_cols, id_cols\n",
    "\n",
    "# --- Initialize Session State ---\n",
    "def initialize_session_state():\n",
    "    \"\"\"Initializes all required keys in Streamlit's session state.\"\"\"\n",
    "    if 'page' not in st.session_state:\n",
    "        st.session_state.page = 'upload'\n",
    "    if 'task' not in st.session_state:\n",
    "        st.session_state.task = None\n",
    "    \n",
    "    if 'df' not in st.session_state: st.session_state.df = None\n",
    "    if 'processed_df' not in st.session_state: st.session_state.processed_df = None\n",
    "    if 'model' not in st.session_state: st.session_state.model = None\n",
    "    if 'forecast_df' not in st.session_state: st.session_state.forecast_df = None\n",
    "\n",
    "    if 'user_selections' not in st.session_state:\n",
    "        st.session_state.user_selections = {}\n",
    "    if 'best_params' not in st.session_state:\n",
    "        st.session_state.best_params = None\n",
    "    if 'cluster_labels' not in st.session_state:\n",
    "        st.session_state.cluster_labels = {}\n",
    "    if 'preprocessing_config' not in st.session_state:\n",
    "        st.session_state.preprocessing_config = {}\n",
    "    if 'showdown_results' not in st.session_state:\n",
    "        st.session_state.showdown_results = None\n",
    "    if 'date_converted' not in st.session_state:\n",
    "        st.session_state.date_converted = False\n",
    "    if 'report_data' not in st.session_state:\n",
    "        st.session_state.report_data = {}\n",
    "\n",
    "\n",
    "# --- Sidebar Navigation ---\n",
    "def render_sidebar():\n",
    "    \"\"\"Renders the sidebar with progress indicators based on the selected task.\"\"\"\n",
    "    \n",
    "    # --- Icon Definitions (Base64 Encoded SVGs) ---\n",
    "    chart_svg_base64 = \"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiM4QTJCRTIiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48bGluZSB4MT0iMTIiIHkxPSIyMCIgeDI9IjEyIiB5Mj0iMTAiPjwvbGluZT48bGluZSB4MT0iMTgiIHkxPSIyMCIgeDI9IjE4IiB5Mj0iNCI+PC9saW5lPjxsaW5lIHgxPSI2IiB5MT0iMjAiIHgyPSI2IiB5Mj0iMTYiPjwvbGluZT48L3N2Zz4=\"\n",
    "    check_svg_base64 = \"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiM0YWRlODAiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMjIgMTEuMDhWMTJhMTAgMTAgMCAxIDEtNS45My05LjE0Ii8+PHBvbHlsaW5lIHBvaW50cz0iMjIgNCAxMiAxNC4wMSA5IDExLjAxIi8+PC9zdmc+\"\n",
    "    cog_svg_base64 = \"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiM4QTJCRTIiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIiBjbGFzcz0iZmVhdGhlciBmZWF0aGVyLXNldHRpbmdzIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIzIj48L2NpcmNsZT48cGF0aCBkPSJNMTkuNCAxNWExLjY1IDEuNjUgMCAwIDAgLjMzIDEuODJsLjA2LjA2YTIgMiAwIDAgMSAwIDIuODMgMiAyIDAgMCAxLTIuODMgMGwtLjA2LS4wNmExLjY1IDEuNjUgMCAwIDAtMS44Mi0uMzMgMS42NSAxLjY1IDAgMCAwLTEgMS41MVYyMWExIDIgMCAwIDEtMiAyIDIgMiAwIDAgMS0yLTJ2LS4wOUExLjY1IDEuNjUgMCAwIDAgOSAxOS40YTEuNjUgMS42NSAwIDAgMC0xLjgyLjMzbC0uMDYuMDZhMiAyIDAgMCAxLTIuODMgMCAyIDIgMCAwIDEgMC0yLjgzbC4wNi0uMDZhMS42NSAxLjY1IDEuNjUgMCAwIDAgLjMzLTEuODIgMS42NSAxLjY1IDAgMCAwLTEuNTEtMUgzYTIgMiAwIDAgMS0yLTIgMiAyIDAgMCAxIDItMmguMDlBMS42NSAxLjY1IDAgMCAwIDQuNiA5YTEuNjUgMS42NSAwIDAgMC0uMzMtMS44MmwtLjA2LS4wNmEyIDIgMCAwIDEgMC0yLjgzIDIgMiAwIDAgMSAyLjgzIDBsLjA2LjA2YTEuNjUgMS42NSAwIDAgMCAxLjgyLjMzSDlhMS42NSAxLjY1IDAgMCAwIDEtMS41MVYzYTIgMiAwIDAgMSAyLTIgMiAyIDAgMCAxIDIgMnYuMDlhMS42NSAxLjY1IDAgMCAwIDEgMS41MSAxLjY1IDEuNjUgMCAwIDAgMS44Mi0uMzNsLjA2LS4wNmEyIDIgMCAwIDEgMi44MyAwIDIgMiAwIDAgMSAwIDIuODNsLS4wNi4wNmExLjY1IDEuNjUgMCAwIDAtLjMzIDEuODJWOWExLjY1IDEuNjUgMCAwIDAgMS41MSAxSDIxYTIgMiAwIDAgMSAyIDIgMiAyIDAgMCAxLTIgMmgtLjA5YTEuNjUgMS42NSAwIDAgMC0xLjUxIDF6Ij48L3BhdGg+PC9zdmc+\"\n",
    "    square_svg_base64 = \"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiM5NGExYjgiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cmVjdCB4PSIzIiB5PSIzIiB3aWR0aD0iMTgiIGhlaWdodD0iMTgiIHJ4PSIyIiByeT0iMiIvPjwvc3ZnPg==\"\n",
    "\n",
    "    title_html = f'<div style=\"display: flex; align-items: center; gap: 15px; margin-bottom: 20px;\"><img src=\"{chart_svg_base64}\" width=\"24\" height=\"24\"> <span style=\"font-size: 1.35rem; font-weight: bold;\">Pipeline Progress</span></div>'\n",
    "    st.sidebar.write(title_html, unsafe_allow_html=True)\n",
    "    \n",
    "    task = st.session_state.get('task')\n",
    "    \n",
    "    if not task:\n",
    "        steps = ['Upload', 'Task Selection']\n",
    "    elif task == \"Predictive Modeling\":\n",
    "        algorithm = st.session_state.get('user_selections', {}).get('algorithm')\n",
    "        if algorithm in [\"Prophet\", \"ARIMA\"]:\n",
    "            steps = ['Upload', 'Task Selection', 'Preprocessing', 'EDA', 'Feature Engineering', 'Model Recommendation', 'Modeling', 'Evaluation']\n",
    "        else:\n",
    "            steps = ['Upload', 'Task Selection', 'Preprocessing', 'EDA', 'Feature Engineering', 'Model Recommendation', 'Modeling', 'Evaluation', 'Action & Export']\n",
    "    elif task == \"Clustering Analysis\":\n",
    "        steps = ['Upload', 'Task Selection', 'Preprocessing', 'EDA', 'Clustering', 'Cluster Analysis', 'Action & Export']\n",
    "    else:\n",
    "        steps = ['Upload']\n",
    "\n",
    "    page_map = {\n",
    "        'upload': 0, 'task_hub': 1, 'preprocessing': 2, 'eda': 3,\n",
    "        'feature_engineering': 4, 'model_recommendation': 5, 'modeling': 6, 'evaluation': 7,\n",
    "        'clustering': 4, 'cluster_analysis': 5, 'action_export': 8\n",
    "    }\n",
    "    \n",
    "    if st.session_state.page == 'action_export':\n",
    "         current_step_index = len(steps) - 1\n",
    "    else:\n",
    "        current_step_index = page_map.get(st.session_state.page, 0)\n",
    "\n",
    "    for i, step in enumerate(steps):\n",
    "        if i < current_step_index:\n",
    "            icon_html = f'<img src=\"{check_svg_base64}\" width=\"16\" height=\"16\" style=\"display:inline-block; vertical-align:middle; margin-right:8px;\">'\n",
    "            st.sidebar.write(f'{icon_html} <span style=\"color:#4ade80; font-size: 16px;\">{step}</span>', unsafe_allow_html=True)\n",
    "        elif i == current_step_index:\n",
    "            icon_html = f'<img src=\"{cog_svg_base64}\" width=\"16\" height=\"16\" style=\"display:inline-block; vertical-align:middle; margin-right:8px;\">'\n",
    "            st.sidebar.write(f'{icon_html} <strong style=\"color:white; font-size: 16px;\">{step}</strong>', unsafe_allow_html=True)\n",
    "        else:\n",
    "            icon_html = f'<img src=\"{square_svg_base64}\" width=\"16\" height=\"16\" style=\"display:inline-block; vertical-align:middle; margin-right:8px;\">'\n",
    "            st.sidebar.write(f'{icon_html} <span style=\"color:#94a3b8; font-size: 16px;\">{step}</span>', unsafe_allow_html=True)\n",
    "    \n",
    "    st.sidebar.markdown(\"<hr style='margin-top:20px; margin-bottom:20px;'>\", unsafe_allow_html=True)\n",
    "\n",
    "    if st.sidebar.button(\"Start Over\", key=\"start_over_sidebar\"):\n",
    "        for key in list(st.session_state.keys()):\n",
    "            del st.session_state[key]\n",
    "        st.rerun()\n",
    "\n",
    "# --- Page Rendering Functions ---\n",
    "\n",
    "def render_upload_page():\n",
    "    st.markdown(\"\"\"\n",
    "        <style>\n",
    "            .stApp > header {\n",
    "                background-color: transparent;\n",
    "            }\n",
    "            .upload-container {\n",
    "                display: flex;\n",
    "                flex-direction: column;\n",
    "                align-items: center;\n",
    "                justify-content: flex-start;\n",
    "                height: 100vh;\n",
    "                padding-top: 5rem;\n",
    "            }\n",
    "            .title {\n",
    "                font-size: 2rem; \n",
    "                font-weight: 700; \n",
    "                color: #E2E8F0; \n",
    "                letter-spacing: 0.1em;\n",
    "                margin-bottom: 4rem;\n",
    "            }\n",
    "            .upload-section {\n",
    "                width: 100%;\n",
    "                max-width: 896px; \n",
    "            }\n",
    "            .upload-header {\n",
    "                font-size: 3rem; \n",
    "                font-weight: bold;\n",
    "                color: #8A2BE2;\n",
    "                margin-bottom: 2rem;\n",
    "            }\n",
    "            .stFileUploader > div > div {\n",
    "                border: 2px dashed #4A5568;\n",
    "                background-color: rgba(45, 55, 72, 0.5);\n",
    "                padding: 3rem; \n",
    "                border-radius: 0.75rem;\n",
    "            }\n",
    "            .stFileUploader > div > div > button {\n",
    "                background-color: #8A2BE2;\n",
    "                color: white;\n",
    "                padding: 0.75rem 1.5rem;\n",
    "                font-size: 1rem;\n",
    "            }\n",
    "            .stFileUploader > div > div > button:hover {\n",
    "                background-color: #6B21A8;\n",
    "            }\n",
    "        </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "    st.markdown('<div class=\"title\" style=\"text-align: center;\">DATA SCIENTIST AI</div>', unsafe_allow_html=True)\n",
    "    \n",
    "    _ , col2, _ = st.columns([1, 3, 1]) \n",
    "    with col2:\n",
    "        st.markdown('<div class=\"upload-section\">', unsafe_allow_html=True)\n",
    "        st.markdown('<div class=\"upload-header\">Upload Data</div>', unsafe_allow_html=True)\n",
    "        \n",
    "        uploaded_file = st.file_uploader(\n",
    "            \"Drag and drop file here\", \n",
    "            type=\"csv\",\n",
    "            label_visibility=\"collapsed\"\n",
    "        )\n",
    "        \n",
    "        st.markdown('</div>', unsafe_allow_html=True)\n",
    "\n",
    "    if uploaded_file:\n",
    "        try:\n",
    "            st.session_state.df = pd.read_csv(uploaded_file)\n",
    "            st.session_state.page = 'task_hub'\n",
    "            st.rerun()\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error loading file: {e}\")\n",
    "\n",
    "def render_task_hub():\n",
    "    st.header(\"Stage 2: Task Selection\")\n",
    "    st.write(\"Your data is loaded. What would you like to do next?\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        st.subheader(\"Predictive Modeling\")\n",
    "        st.info(\"Forecast future values, classify outcomes, or predict numerical targets.\")\n",
    "        if st.button(\"Start Predictive Modeling\"):\n",
    "            st.session_state.task = \"Predictive Modeling\"\n",
    "            df = st.session_state.df.copy()\n",
    "            # Automatic date conversion runs only once here\n",
    "            for col in df.columns:\n",
    "                if col.lower() == 'date':\n",
    "                    try:\n",
    "                        df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)\n",
    "                        st.toast(f\"Automatically converted column '{col}' to datetime.\", icon=\"ð\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            st.session_state.processed_df = df\n",
    "            st.session_state.page = 'preprocessing'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        st.subheader(\"Clustering Analysis\")\n",
    "        st.info(\"Automatically discover hidden groups or segments in your data.\")\n",
    "        if st.button(\"Start Clustering Analysis\"):\n",
    "            st.session_state.task = \"Clustering Analysis\"\n",
    "            st.session_state.processed_df = st.session_state.df.copy()\n",
    "            st.session_state.page = 'preprocessing'\n",
    "            st.rerun()\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    if st.button(\"Go Back to Upload\"):\n",
    "        st.session_state.page = 'upload'\n",
    "        st.session_state.df = None # Allow re-upload\n",
    "        st.rerun()\n",
    "\n",
    "def render_preprocessing_page():\n",
    "    st.header(\"Configure Cleaning & Feature Engineering\")\n",
    "    df = st.session_state.processed_df.copy()\n",
    "\n",
    "    # --- NEW: Data Quality Section ---\n",
    "    st.subheader(\"Data Quality Check\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        missing_values = df.isnull().sum()\n",
    "        missing_values = missing_values[missing_values > 0]\n",
    "        if not missing_values.empty:\n",
    "            st.write(\"Columns with Missing Values:\")\n",
    "            st.dataframe(missing_values.to_frame(name='Missing Count'))\n",
    "        else:\n",
    "            st.success(\"No missing values found.\")\n",
    "    \n",
    "    with col2:\n",
    "        num_duplicates = df.duplicated().sum()\n",
    "        st.metric(\"Duplicate Rows Found\", num_duplicates)\n",
    "        if num_duplicates > 0:\n",
    "            primary_key_cols = st.multiselect(\"Select Primary Key column(s) to remove duplicates:\", df.columns)\n",
    "        else:\n",
    "            primary_key_cols = None\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "\n",
    "    # Initialize config if not present or columns have changed\n",
    "    if not st.session_state.preprocessing_config or set(st.session_state.preprocessing_config.keys()) != set(df.columns):\n",
    "        config = {}\n",
    "        numerical_cols, categorical_cols, date_cols, _ = get_column_types(df)\n",
    "        for col in df.columns:\n",
    "            col_type = 'Date' if col in date_cols else ('Numerical' if col in numerical_cols else 'Categorical')\n",
    "            config[col] = {\n",
    "                'type': col_type,\n",
    "                'missing': 'None',\n",
    "                'scaling': 'None' if col_type == 'Numerical' else 'N/A',\n",
    "                'remove': False\n",
    "            }\n",
    "        st.session_state.preprocessing_config = config\n",
    "    \n",
    "    config = st.session_state.preprocessing_config\n",
    "    \n",
    "    header_cols = st.columns([3, 2, 2, 2, 1])\n",
    "    headers = [\"COLUMN\", \"TYPE\", \"MISSING\", \"SCALING\", \"REMOVE\"]\n",
    "    for col, header in zip(header_cols, headers):\n",
    "        col.write(f\"**{header}**\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        row_cols = st.columns([3, 2, 2, 2, 1])\n",
    "        row_cols[0].write(col_name)\n",
    "        row_cols[1].info(config[col_name]['type'])\n",
    "        \n",
    "        missing_options = [\"None\", \"Mean\", \"Median\"] if config[col_name]['type'] == 'Numerical' else [\"None\", \"Mode\"]\n",
    "        if config[col_name]['type'] == 'Date': missing_options = [\"None\"]\n",
    "        config[col_name]['missing'] = row_cols[2].selectbox(\"Missing\", missing_options, key=f\"missing_{col_name}\", label_visibility=\"collapsed\")\n",
    "        \n",
    "        scaling_options = [\"None\", \"StandardScaler\", \"MinMaxScaler\"]\n",
    "        if config[col_name]['type'] == 'Numerical':\n",
    "            config[col_name]['scaling'] = row_cols[3].selectbox(\"Scaling\", scaling_options, key=f\"scaling_{col_name}\", label_visibility=\"collapsed\")\n",
    "        else:\n",
    "            row_cols[3].write(\"N/A\")\n",
    "            \n",
    "        config[col_name]['remove'] = row_cols[4].checkbox(\"\", key=f\"remove_{col_name}\", value=config[col_name]['remove'], label_visibility=\"collapsed\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"Global Encoding Strategy\")\n",
    "    encoding_strategy = st.selectbox(\"Choose how to handle all categorical columns:\", [\"Label Encoding\", \"One-Hot Encoding\"])\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if st.button(\"Go Back\"):\n",
    "            st.session_state.page = 'task_hub'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        if st.button(\"Apply Preprocessing & Proceed\"):\n",
    "            with st.spinner(\"Processing...\"):\n",
    "                proc_df = df.copy()\n",
    "                \n",
    "                if primary_key_cols:\n",
    "                    original_rows = len(proc_df)\n",
    "                    proc_df.drop_duplicates(subset=primary_key_cols, inplace=True)\n",
    "                    st.toast(f\"Removed {original_rows - len(proc_df)} duplicates.\")\n",
    "\n",
    "                cols_to_remove = [col for col, settings in config.items() if settings['remove']]\n",
    "                proc_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "\n",
    "                for col, settings in config.items():\n",
    "                    if col in proc_df.columns:\n",
    "                        if settings['missing'] != 'None':\n",
    "                            if settings['type'] == 'Numerical':\n",
    "                                if settings['missing'] == 'Mean': proc_df[col].fillna(proc_df[col].mean(), inplace=True)\n",
    "                                elif settings['missing'] == 'Median': proc_df[col].fillna(proc_df[col].median(), inplace=True)\n",
    "                            elif settings['type'] == 'Categorical':\n",
    "                                 if settings['missing'] == 'Mode': proc_df[col].fillna(proc_df[col].mode()[0], inplace=True)\n",
    "                \n",
    "                st.session_state.original_processed_df = proc_df.copy()\n",
    "\n",
    "                categorical_cols_to_encode = [col for col, settings in config.items() if col in proc_df.columns and settings['type'] == 'Categorical']\n",
    "                if encoding_strategy == 'Label Encoding':\n",
    "                    for col in categorical_cols_to_encode:\n",
    "                        proc_df[col] = LabelEncoder().fit_transform(proc_df[col].astype(str))\n",
    "                elif encoding_strategy == 'One-Hot Encoding':\n",
    "                    proc_df = pd.get_dummies(proc_df, columns=categorical_cols_to_encode)\n",
    "\n",
    "                for col, settings in config.items():\n",
    "                     if col in proc_df.columns and settings['type'] == 'Numerical' and settings['scaling'] != 'None':\n",
    "                        scaler = StandardScaler() if settings['scaling'] == 'StandardScaler' else MinMaxScaler()\n",
    "                        proc_df[[col]] = scaler.fit_transform(proc_df[[col]])\n",
    "                \n",
    "                proc_df.dropna(inplace=True)\n",
    "                st.session_state.processed_df = proc_df\n",
    "                st.session_state.page = 'eda'\n",
    "                st.rerun()\n",
    "\n",
    "def render_eda_page():\n",
    "    st.header(\"Exploratory Data Analysis (EDA)\")\n",
    "    df = st.session_state.original_processed_df.copy()\n",
    "    report_data = st.session_state.get('report_data', {})\n",
    "    \n",
    "    st.subheader(\"1. Univariate Analysis\")\n",
    "    col1_uni, col2_uni = st.columns([1,2])\n",
    "    with col1_uni:\n",
    "        feature_uni = st.selectbox(\"Select a feature to visualize its distribution:\", df.columns)\n",
    "    with col2_uni:\n",
    "        is_numeric_uni = pd.api.types.is_numeric_dtype(df[feature_uni].dtype)\n",
    "        if is_numeric_uni:\n",
    "            fig = px.histogram(df, x=feature_uni, title=f\"Distribution of {feature_uni}\", template='plotly_dark')\n",
    "        else:\n",
    "            fig = px.bar(df[feature_uni].value_counts(), title=f\"Count of {feature_uni}\", template='plotly_dark')\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "        report_data['univariate_data'] = (df[feature_uni], is_numeric_uni)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"2. Bivariate Analysis\")\n",
    "    col1_bi, col2_bi = st.columns(2)\n",
    "    with col1_bi:\n",
    "        x_axis = st.selectbox(\"Select X-Axis Feature:\", df.columns, key=\"x_axis\")\n",
    "    with col2_bi:\n",
    "        y_axis = st.selectbox(\"Select Y-Axis Feature:\", df.columns, key=\"y_axis\")\n",
    "    \n",
    "    x_is_num = pd.api.types.is_numeric_dtype(df[x_axis].dtype)\n",
    "    y_is_num = pd.api.types.is_numeric_dtype(df[y_axis].dtype)\n",
    "    \n",
    "    if x_axis != y_axis:\n",
    "        if x_is_num and y_is_num:\n",
    "            fig_bi = px.scatter(df, x=x_axis, y=y_axis, title=f\"{x_axis} vs. {y_axis}\", template='plotly_dark')\n",
    "        elif (x_is_num and not y_is_num):\n",
    "            fig_bi = px.box(df, x=y_axis, y=x_axis, title=f\"{y_axis} vs. {x_axis}\", template='plotly_dark')\n",
    "        elif (not x_is_num and y_is_num):\n",
    "            fig_bi = px.box(df, x=x_axis, y=y_axis, title=f\"{x_axis} vs. {y_axis}\", template='plotly_dark')\n",
    "        else:\n",
    "            pivot = df.groupby([x_axis, y_axis]).size().reset_index(name='count')\n",
    "            fig_bi = px.bar(pivot, x=x_axis, y='count', color=y_axis, barmode='group', title=f\"{x_axis} vs. {y_axis}\", template='plotly_dark')\n",
    "        st.plotly_chart(fig_bi, use_container_width=True)\n",
    "        report_data['bivariate_data'] = (df, x_axis, y_axis)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"3. Correlation Heatmap\")\n",
    "    num_df = df.select_dtypes(include=np.number)\n",
    "    if not num_df.empty:\n",
    "        corr = num_df.corr()\n",
    "        fig_corr = px.imshow(corr, text_auto=True, aspect=\"auto\", title=\"Correlation Matrix of Numerical Features\", template='plotly_dark')\n",
    "        st.plotly_chart(fig_corr, use_container_width=True)\n",
    "        report_data['correlation_data'] = corr\n",
    "    \n",
    "    st.session_state.report_data = report_data\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if st.button(\"Go Back\"):\n",
    "            st.session_state.page = 'preprocessing'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        if st.button(\"Proceed\"):\n",
    "            if st.session_state.task == \"Predictive Modeling\":\n",
    "                st.session_state.page = 'feature_engineering'\n",
    "            else:\n",
    "                st.session_state.page = 'clustering'\n",
    "            st.rerun()\n",
    "\n",
    "def render_feature_engineering_page():\n",
    "    st.header(\"Feature Engineering (for Predictive Modeling)\")\n",
    "    df = st.session_state.processed_df\n",
    "    st.write(\"#### Current Data Preview\"); st.dataframe(df.head())\n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    _, categorical_cols, date_cols, _ = get_column_types(st.session_state.original_processed_df)\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        st.subheader(\"Time-Series Features\")\n",
    "        if date_cols:\n",
    "            time_index = st.selectbox(\"Select Time Index Column:\", [\"None\"] + date_cols)\n",
    "            extract_parts = st.multiselect(\"Extract Date Parts:\", ['Year', 'Quarter', 'Month', 'Week', 'Day', 'Dayofweek']) if time_index != \"None\" else []\n",
    "            segment_by = st.selectbox(\"Segment Forecast By (Optional):\", [\"None\"] + categorical_cols)\n",
    "        else:\n",
    "            st.info(\"No date columns were detected in the preprocessed data.\")\n",
    "            time_index, extract_parts, segment_by = \"None\", [], \"None\"\n",
    "    with col2:\n",
    "        st.subheader(\"Lag & Rolling Features\")\n",
    "        num_cols = [c for c in df.select_dtypes(include=np.number).columns]\n",
    "        feature_col = st.selectbox(\"Select column for Lag/Rolling:\", [\"None\"] + num_cols)\n",
    "        if feature_col != \"None\":\n",
    "            lag_periods = st.text_input(\"Lag Periods (comma-separated)\", \"1, 7\")\n",
    "            rolling_window = st.number_input(\"Rolling Window Size\", min_value=0, value=7)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"Feature Selection\")\n",
    "    \n",
    "    col_fs1, col_fs2 = st.columns(2)\n",
    "    with col_fs1:\n",
    "        st.write(\"#### Manual Selection\")\n",
    "        manual_features = st.multiselect(\"Manually select features to keep:\", df.columns, help=\"These features will always be included.\")\n",
    "    \n",
    "    with col_fs2:\n",
    "        st.write(\"#### Automated Selection\")\n",
    "        enable_rfe = st.checkbox(\"Enable Automated Feature Selection (RFE)\")\n",
    "        if enable_rfe:\n",
    "            num_features_to_select = st.slider(\"Number of top features to select:\", 1, len(df.columns)-1, min(10, len(df.columns)-1))\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if st.button(\"Go Back\"):\n",
    "            st.session_state.page = 'eda'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        if st.button(\"Apply Features & Proceed\"):\n",
    "            with st.spinner(\"Engineering and Selecting Features...\"):\n",
    "                proc_df = df.copy()\n",
    "                original_df = st.session_state.original_processed_df.copy()\n",
    "                st.session_state.user_selections.update({\n",
    "                    'time_index': time_index, 'date_parts': extract_parts, 'segment_by': segment_by,\n",
    "                    'feature_eng_col': feature_col,\n",
    "                    'lag_periods': lag_periods if feature_col != \"None\" else \"\", 'rolling_window': rolling_window if feature_col != \"None\" else 0,\n",
    "                    'manual_features': manual_features,\n",
    "                    'rfe_enabled': enable_rfe, 'rfe_features': num_features_to_select if enable_rfe else 0\n",
    "                })\n",
    "                if time_index != \"None\" and time_index in original_df.columns:\n",
    "                    proc_df[time_index] = pd.to_datetime(original_df[time_index])\n",
    "                    if 'Year' in extract_parts: proc_df['Year'] = proc_df[time_index].dt.year\n",
    "                    if 'Quarter' in extract_parts: proc_df['Quarter'] = proc_df[time_index].dt.quarter\n",
    "                    if 'Month' in extract_parts: proc_df['Month'] = proc_df[time_index].dt.month\n",
    "                    proc_df.drop(columns=[time_index], inplace=True, errors='ignore')\n",
    "\n",
    "                if feature_col != \"None\":\n",
    "                    if lag_periods:\n",
    "                        periods = [int(p.strip()) for p in lag_periods.split(',') if p.strip()]\n",
    "                        for p in periods:\n",
    "                            proc_df[f'{feature_col}_lag_{p}'] = proc_df[feature_col].shift(p)\n",
    "                    if rolling_window > 0:\n",
    "                        proc_df[f'{feature_col}_roll_mean_{rolling_window}'] = proc_df[feature_col].rolling(window=rolling_window).mean()\n",
    "                \n",
    "                proc_df.dropna(inplace=True)\n",
    "                st.session_state.processed_df = proc_df\n",
    "                st.session_state.page = 'model_recommendation'\n",
    "                st.rerun()\n",
    "\n",
    "def render_recommendation_page():\n",
    "    st.header(\"Model Recommendation\")\n",
    "    df = st.session_state.processed_df\n",
    "    selections = st.session_state.user_selections\n",
    "    \n",
    "    is_time_series = selections.get('time_index') != 'None'\n",
    "    \n",
    "    if is_time_series:\n",
    "        st.success(\"**Top Recommendation: Prophet**\")\n",
    "        st.info(\"Because you selected a time index, your goal is likely forecasting. Prophet is a specialized forecasting model that is robust to missing data and shifts in trend, and typically handles seasonality well.\")\n",
    "        st.warning(\"**Secondary Option: ARIMA**\")\n",
    "        st.info(\"ARIMA is another powerful statistical model for time-series data.\")\n",
    "    else:\n",
    "        st.info(\"This is a data-driven model showdown. We train several models on a sample of your data to see which performs best, giving you an intelligent starting point.\")\n",
    "        target_column = st.selectbox(\"Select your target column for the showdown:\", df.columns, index=len(df.columns)-1)\n",
    "\n",
    "        if st.button(\"Find Best Model for My Data\"):\n",
    "            with st.spinner(\"Running model showdown... This may take a moment.\"):\n",
    "                problem_type = \"Regression\" if pd.api.types.is_numeric_dtype(df[target_column].dtype) else \"Classification\"\n",
    "                \n",
    "                sample_df = df.select_dtypes(exclude=['datetime', 'datetimetz']).sample(n=min(len(df), 1000), random_state=42)\n",
    "                \n",
    "                for col in sample_df.select_dtypes(include=['object', 'category']).columns:\n",
    "                    sample_df[col] = LabelEncoder().fit_transform(sample_df[col].astype(str))\n",
    "\n",
    "                X = sample_df.drop(columns=[target_column])\n",
    "                y = sample_df[target_column]\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "                \n",
    "                models_to_test = {\n",
    "                    \"Linear/Logistic Regression\": LinearRegression() if problem_type == \"Regression\" else LogisticRegression(),\n",
    "                    \"Random Forest\": RandomForestRegressor(random_state=42) if problem_type == \"Regression\" else RandomForestClassifier(random_state=42),\n",
    "                }\n",
    "                if XGB_AVAILABLE:\n",
    "                    models_to_test[\"XGBoost\"] = xgb.XGBRegressor(random_state=42) if problem_type == \"Regression\" else xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "                results = []\n",
    "                for name, model in models_to_test.items():\n",
    "                    model.fit(X_train, y_train)\n",
    "                    preds = model.predict(X_test)\n",
    "                    if problem_type == \"Regression\":\n",
    "                        score = r2_score(y_test, preds)\n",
    "                        metric_name = \"R-Squared\"\n",
    "                    else:\n",
    "                        score = accuracy_score(y_test, preds)\n",
    "                        metric_name = \"Accuracy\"\n",
    "                    results.append({\"Model\": name, metric_name: score})\n",
    "                \n",
    "                results_df = pd.DataFrame(results).sort_values(by=metric_name, ascending=False).reset_index(drop=True)\n",
    "                st.session_state.showdown_results = results_df\n",
    "                \n",
    "    if st.session_state.showdown_results is not None:\n",
    "        st.subheader(\"Model Showdown Leaderboard\")\n",
    "        st.dataframe(st.session_state.showdown_results)\n",
    "        top_model = st.session_state.showdown_results.iloc[0][\"Model\"]\n",
    "        st.success(f\"**Top Recommendation:** Based on the showdown, **{top_model}** is the recommended model.\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.write(\"You can accept the recommendation or choose any model on the next page.\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if st.button(\"Go Back\"):\n",
    "            st.session_state.page = 'feature_engineering'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        if st.button(\"Proceed to Modeling\"):\n",
    "            st.session_state.page = 'modeling'\n",
    "            st.rerun()\n",
    "\n",
    "def render_modeling_page():\n",
    "    st.header(\"Modeling (Predictive)\")\n",
    "    df = st.session_state.processed_df\n",
    "    selections = st.session_state.user_selections\n",
    "    \n",
    "    target_column = st.selectbox(\"Select Target Column:\", df.columns, index=len(df.columns)-1, key=\"target_select\")\n",
    "    problem_type = \"Regression\" if pd.api.types.is_numeric_dtype(df[target_column].dtype) else \"Classification\"\n",
    "    st.session_state.problem_type = problem_type\n",
    "\n",
    "    if selections.get('rfe_enabled') and 'rfe_ran' not in st.session_state:\n",
    "        with st.spinner(\"Running Recursive Feature Elimination...\"):\n",
    "            temp_df = df.select_dtypes(exclude=['datetime', 'datetimetz']).dropna()\n",
    "            X_temp = temp_df.drop(columns=[target_column])\n",
    "            y_temp = temp_df[target_column]\n",
    "            estimator = RandomForestRegressor(n_estimators=50, random_state=42) if problem_type == \"Regression\" else RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "            rfe = RFE(estimator, n_features_to_select=selections['rfe_features'])\n",
    "            rfe.fit(X_temp, y_temp)\n",
    "            \n",
    "            rfe_selected_features = list(X_temp.columns[rfe.support_])\n",
    "            manual_features = selections.get('manual_features', [])\n",
    "            \n",
    "            final_features = list(set(rfe_selected_features + manual_features))\n",
    "            \n",
    "            df = df[final_features + [target_column]]\n",
    "            st.session_state.processed_df = df\n",
    "            st.session_state['rfe_ran'] = True\n",
    "            st.success(f\"RFE Complete. Final features selected: {', '.join(final_features)}\")\n",
    "\n",
    "    st.write(\"#### Final Data for Modeling\"); st.dataframe(df.head())\n",
    "    st.markdown(\"---\")\n",
    "\n",
    "    ml_algorithms = [\"Random Forest\", \"Gradient Boosting\", \"Support Vector Machine (SVM)\", \"Linear/Logistic Regression\"]\n",
    "    if XGB_AVAILABLE: ml_algorithms.append(\"XGBoost\")\n",
    "    if LGBM_AVAILABLE: ml_algorithms.append(\"LightGBM\")\n",
    "    if CATBOOST_AVAILABLE: ml_algorithms.append(\"CatBoost\")\n",
    "    ts_algorithms = [\"Prophet\", \"ARIMA\"] if PROPHET_AVAILABLE else []\n",
    "    \n",
    "    algorithm = st.selectbox(\"Select Algorithm:\", ml_algorithms + ts_algorithms, key=\"algo_select\")\n",
    "    \n",
    "    is_ts_model = algorithm in ts_algorithms\n",
    "    if is_ts_model:\n",
    "        if selections.get('time_index') == 'None':\n",
    "            st.error(\"ARIMA and Prophet require a time index column.\")\n",
    "            return\n",
    "        if problem_type == \"Classification\":\n",
    "            st.error(\"ARIMA and Prophet are for regression problems only.\")\n",
    "            return\n",
    "\n",
    "    if st.button(\"Train Model\"):\n",
    "        st.session_state.user_selections['target'] = target_column\n",
    "        st.session_state.user_selections['algorithm'] = algorithm\n",
    "        \n",
    "        with st.spinner(f\"Training {algorithm} model...\"):\n",
    "            if is_ts_model:\n",
    "                models = {}\n",
    "                # Train overall model\n",
    "                ts_df = st.session_state.df.copy()\n",
    "                time_col = selections['time_index']\n",
    "                ts_df['ds'] = pd.to_datetime(ts_df[time_col], dayfirst=True)\n",
    "                ts_df.rename(columns={target_column: 'y'}, inplace=True)\n",
    "                \n",
    "                if algorithm == \"Prophet\":\n",
    "                    model_overall = Prophet().fit(ts_df[['ds', 'y']])\n",
    "                elif algorithm == \"ARIMA\":\n",
    "                    model_overall = ARIMA(ts_df['y'].values, order=(5,1,0)).fit()\n",
    "                models['Overall'] = model_overall\n",
    "\n",
    "                # Train segmented models if applicable\n",
    "                segment_by = selections.get('segment_by')\n",
    "                if segment_by and segment_by != \"None\":\n",
    "                    for segment_value in st.session_state.df[segment_by].unique():\n",
    "                        segment_df = st.session_state.df[st.session_state.df[segment_by] == segment_value].copy()\n",
    "                        segment_df['ds'] = pd.to_datetime(segment_df[time_col], dayfirst=True)\n",
    "                        segment_df.rename(columns={target_column: 'y'}, inplace=True)\n",
    "                        if algorithm == \"Prophet\":\n",
    "                            model_segment = Prophet().fit(segment_df[['ds', 'y']])\n",
    "                        elif algorithm == \"ARIMA\":\n",
    "                            model_segment = ARIMA(segment_df['y'].values, order=(5,1,0)).fit()\n",
    "                        models[segment_value] = model_segment\n",
    "                st.session_state.model = models\n",
    "            else:\n",
    "                X = df.select_dtypes(exclude=['datetime', 'datetimetz']).drop(columns=[target_column])\n",
    "                y = df[target_column]\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                \n",
    "                model_options = {\n",
    "                    \"Random Forest\": RandomForestRegressor(random_state=42) if problem_type == \"Regression\" else RandomForestClassifier(random_state=42),\n",
    "                    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42) if problem_type == \"Regression\" else GradientBoostingClassifier(random_state=42),\n",
    "                    \"Support Vector Machine (SVM)\": SVR() if problem_type == \"Regression\" else SVC(random_state=42, probability=True),\n",
    "                    \"Linear/Logistic Regression\": LinearRegression() if problem_type == \"Regression\" else LogisticRegression(random_state=42, max_iter=1000),\n",
    "                    \"XGBoost\": xgb.XGBRegressor(random_state=42) if problem_type == \"Regression\" else xgb.XGBClassifier(random_state=42),\n",
    "                    \"LightGBM\": lgb.LGBMRegressor(random_state=42) if problem_type == \"Regression\" else lgb.LGBMClassifier(random_state=42),\n",
    "                    \"CatBoost\": cb.CatBoostRegressor(random_state=42, verbose=0) if problem_type == \"Regression\" else cb.CatBoostClassifier(random_state=42, verbose=0)\n",
    "                }\n",
    "                model = model_options[algorithm]\n",
    "                model.fit(X_train, y_train)\n",
    "                st.session_state.model = model\n",
    "                st.session_state.X_test = X_test\n",
    "                st.session_state.y_test = y_test\n",
    "                \n",
    "                st.subheader(\"Quick Test Set Performance\")\n",
    "                predictions = model.predict(X_test)\n",
    "                if problem_type == \"Regression\":\n",
    "                    plot_df = pd.DataFrame({'Actual': y_test.values, 'Predicted': predictions}).reset_index()\n",
    "                    fig_pred = px.line(plot_df, x='index', y=['Actual', 'Predicted'], title='Actual vs. Predicted on Test Set', template='plotly_dark')\n",
    "                    st.plotly_chart(fig_pred, use_container_width=True)\n",
    "            \n",
    "            st.success(f\"{algorithm} model(s) trained successfully!\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if st.button(\"Go Back\"):\n",
    "            st.session_state.page = 'model_recommendation'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        if st.button(\"Proceed to Full Evaluation\"):\n",
    "            st.session_state.page = 'evaluation'\n",
    "            st.rerun()\n",
    "\n",
    "def render_evaluation_page():\n",
    "    st.header(\"Evaluation & Explainability (Predictive)\")\n",
    "    model_or_models = st.session_state.model\n",
    "    selections = st.session_state.user_selections\n",
    "    algorithm = selections.get('algorithm', 'Unknown Model')\n",
    "    is_ts_model = algorithm in [\"Prophet\", \"ARIMA\"]\n",
    "    report_data = st.session_state.get('report_data', {})\n",
    "    report_data['segment_forecasts'] = {}\n",
    "\n",
    "    if is_ts_model:\n",
    "        tab1, tab2, tab3 = st.tabs([\"ð Forecast\", \"ð Components\", \"ð Automated Report\"])\n",
    "        with tab1:\n",
    "            st.subheader(\"Overall Forecast\")\n",
    "            overall_model = model_or_models['Overall']\n",
    "            with st.spinner(f\"Generating overall forecast plot...\"):\n",
    "                if algorithm == \"Prophet\":\n",
    "                    horizon = selections.get('forecast_horizon', 365)\n",
    "                    freq = selections.get('forecast_freq', 'D')\n",
    "                    future = overall_model.make_future_dataframe(periods=horizon, freq=freq)\n",
    "                    forecast = overall_model.predict(future)\n",
    "                    st.session_state.forecast_df = forecast\n",
    "                    \n",
    "                    fig = go.Figure()\n",
    "                    fig.add_trace(go.Scatter(x=overall_model.history['ds'].tail(365), y=overall_model.history['y'].tail(365), mode='lines', name='Historical (Last Year)'))\n",
    "                    fig.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat'], mode='lines', name='Forecast'))\n",
    "                    fig.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat_upper'], fill=None, mode='lines', line_color='rgba(0,0,0,0)'))\n",
    "                    fig.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat_lower'], fill='tonexty', mode='lines', line_color='rgba(0,0,0,0)'))\n",
    "                    st.plotly_chart(fig, use_container_width=True)\n",
    "                    report_data['forecast_fig'] = (overall_model.history, forecast)\n",
    "            \n",
    "            if len(model_or_models) > 1:\n",
    "                st.subheader(\"Forecast Summary by Segment\")\n",
    "                forecast_summaries = []\n",
    "                for segment, model in model_or_models.items():\n",
    "                    if segment != 'Overall' and algorithm == \"Prophet\":\n",
    "                        future = model.make_future_dataframe(periods=selections.get('forecast_horizon', 365), freq=selections.get('forecast_freq', 'D'))\n",
    "                        forecast = model.predict(future)\n",
    "                        total_forecast = forecast['yhat'].sum()\n",
    "                        forecast_summaries.append({'Segment': segment, 'Total Forecasted Value': total_forecast})\n",
    "                \n",
    "                if forecast_summaries:\n",
    "                    summary_df = pd.DataFrame(forecast_summaries)\n",
    "                    fig_bar = px.bar(summary_df, x='Segment', y='Total Forecasted Value', title='Total Forecasted Value by Segment', template='plotly_dark')\n",
    "                    st.plotly_chart(fig_bar, use_container_width=True)\n",
    "                    report_data['segment_summary_bar'] = summary_df\n",
    "\n",
    "                with st.expander(\"View Detailed Forecasts by Segment\"):\n",
    "                    for segment, model in model_or_models.items():\n",
    "                        if segment != 'Overall':\n",
    "                            st.write(f\"### Forecast for: {segment}\")\n",
    "                            with st.spinner(f\"Generating forecast plot for {segment}...\"):\n",
    "                                if algorithm == \"Prophet\":\n",
    "                                    future = model.make_future_dataframe(periods=selections.get('forecast_horizon', 365), freq=selections.get('forecast_freq', 'D'))\n",
    "                                    forecast = model.predict(future)\n",
    "                                    \n",
    "                                    fig_segment = go.Figure()\n",
    "                                    fig_segment.add_trace(go.Scatter(x=model.history['ds'].tail(365), y=model.history['y'].tail(365), mode='lines', name='Historical'))\n",
    "                                    fig_segment.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat'], mode='lines', name='Forecast'))\n",
    "                                    fig_segment.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat_upper'], fill=None, mode='lines', line_color='rgba(0,0,0,0)'))\n",
    "                                    fig_segment.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat_lower'], fill='tonexty', mode='lines', line_color='rgba(0,0,0,0)'))\n",
    "                                    st.plotly_chart(fig_segment, use_container_width=True)\n",
    "                                    report_data['segment_forecasts'][segment] = (model.history, forecast)\n",
    "        with tab2:\n",
    "            st.subheader(\"Forecast Components\")\n",
    "            if algorithm == \"Prophet\":\n",
    "                fig_components = plot_components_plotly(model_or_models['Overall'], st.session_state.forecast_df)\n",
    "                st.plotly_chart(fig_components, use_container_width=True)\n",
    "                report_data['components_fig'] = (model_or_models['Overall'], st.session_state.forecast_df)\n",
    "\n",
    "        with tab3:\n",
    "            st.subheader(\"Export Artifacts\")\n",
    "            if st.session_state.forecast_df is not None:\n",
    "                forecast_csv = st.session_state.forecast_df.to_csv().encode('utf-8')\n",
    "                st.download_button(\"Download Forecast Data (CSV)\", forecast_csv, \"forecast_data.csv\", \"text/csv\")\n",
    "\n",
    "            if st.session_state.model:\n",
    "                model_pkl = pickle.dumps(st.session_state.model)\n",
    "                st.download_button(\"Download Trained Model (PKL)\", model_pkl, \"model.pkl\")\n",
    "            \n",
    "            st.download_button(\"Download Full Report (PDF)\", create_pdf_report(report_data), \"report.pdf\")\n",
    "\n",
    "    else:\n",
    "        tab1, tab2 = st.tabs([\"ð Dashboard\", \"ð Automated Report\"])\n",
    "        with tab1:\n",
    "            st.subheader(\"Performance Metrics\")\n",
    "            X_test, y_test = st.session_state.X_test, st.session_state.y_test\n",
    "            predictions = model_or_models.predict(X_test)\n",
    "            metrics = {}\n",
    "            if st.session_state.problem_type == \"Regression\":\n",
    "                metrics['R-Squared'] = r2_score(y_test, predictions)\n",
    "                metrics['MAE'] = mean_absolute_error(y_test, predictions)\n",
    "            else:\n",
    "                metrics['Accuracy'] = accuracy_score(y_test, predictions)\n",
    "                metrics['F1-Score'] = f1_score(y_test, predictions, average='weighted')\n",
    "            st.session_state.metrics = metrics\n",
    "            metric_cols = st.columns(len(metrics))\n",
    "            for i, (key, value) in enumerate(metrics.items()):\n",
    "                metric_cols[i].metric(key, f\"{value:.4f}\")\n",
    "            \n",
    "            if st.session_state.problem_type == \"Classification\":\n",
    "                st.subheader(\"ROC Curve\")\n",
    "                y_pred_proba = model_or_models.predict_proba(st.session_state.X_test)[:, 1]\n",
    "                fpr, tpr, _ = roc_curve(st.session_state.y_test, y_pred_proba)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                fig_roc = go.Figure(data=go.Scatter(x=fpr, y=tpr, mode='lines', name=f'ROC curve (area = {roc_auc:.2f})'))\n",
    "                fig_roc.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Chance', line=dict(dash='dash')))\n",
    "                st.plotly_chart(fig_roc, use_container_width=True)\n",
    "                report_data['roc_curve'] = (fpr, tpr, roc_auc)\n",
    "        \n",
    "        with tab2:\n",
    "            st.subheader(\"Export Artifacts\")\n",
    "            processed_csv = st.session_state.processed_df.to_csv().encode('utf-8')\n",
    "            st.download_button(\"Download Processed Data (CSV)\", processed_csv, \"processed_data.csv\", \"text/csv\")\n",
    "\n",
    "            if st.session_state.model:\n",
    "                model_pkl = pickle.dumps(st.session_state.model)\n",
    "                st.download_button(\"Download Trained Model (PKL)\", model_pkl, \"model.pkl\")\n",
    "            \n",
    "            st.download_button(\"Download Full Report (PDF)\", create_pdf_report(report_data), \"report.pdf\")\n",
    "    \n",
    "    st.session_state.report_data = report_data\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if st.button(\"Go Back\"):\n",
    "            st.session_state.page = 'modeling'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        if st.button(\"Proceed to Action & Export\"):\n",
    "            st.session_state.page = 'action_export'\n",
    "            st.rerun()\n",
    "\n",
    "def render_clustering_page():\n",
    "    st.header(\"Clustering Model (K-Means)\")\n",
    "    df = st.session_state.processed_df\n",
    "    st.write(\"#### Data for Clustering\"); st.dataframe(df.head())\n",
    "    \n",
    "    col1, col2 = st.columns([2,1])\n",
    "    with col1:\n",
    "        st.subheader(\"Find Optimal Number of Clusters (Elbow Method)\")\n",
    "        with st.spinner(\"Calculating inertia...\"):\n",
    "            inertia = []\n",
    "            k_range = range(1, 11)\n",
    "            for k in k_range:\n",
    "                kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "                kmeans.fit(df)\n",
    "                inertia.append(kmeans.inertia_)\n",
    "            fig = px.line(x=k_range, y=inertia, title=\"Elbow Method for Optimal k\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "    with col2:\n",
    "        k = st.number_input(\"Select Number of Clusters (k)\", 2, 20, 3)\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if st.button(\"Go Back\"):\n",
    "            st.session_state.page = 'eda'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        if st.button(\"Run Clustering & Analyze Results\"):\n",
    "            kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "            df['Cluster'] = kmeans.fit_predict(df)\n",
    "            st.session_state.processed_df = df\n",
    "            st.session_state.model = kmeans\n",
    "            st.session_state.page = 'cluster_analysis'\n",
    "            st.rerun()\n",
    "\n",
    "def render_cluster_analysis_page():\n",
    "    st.header(\"Cluster Analysis\")\n",
    "    df = st.session_state.processed_df\n",
    "    report_data = st.session_state.get('report_data', {})\n",
    "    \n",
    "    st.subheader(\"Cluster Population\")\n",
    "    fig_bar = px.bar(df['Cluster'].value_counts(), title=\"Cluster Population\", template='plotly_dark')\n",
    "    st.plotly_chart(fig_bar, use_container_width=True)\n",
    "    report_data['cluster_population_data'] = df['Cluster'].value_counts()\n",
    "    \n",
    "    st.subheader(\"Cluster Scatter Plot\")\n",
    "    all_cols = [col for col in df.columns if col != 'Cluster']\n",
    "    x_axis = st.selectbox(\"X-Axis\", all_cols, index=0)\n",
    "    y_axis = st.selectbox(\"Y-Axis\", all_cols, index=1)\n",
    "    fig_scatter = px.scatter(df, x=x_axis, y=y_axis, color=\"Cluster\", title=f\"{x_axis} vs. {y_axis} by Cluster\")\n",
    "    st.plotly_chart(fig_scatter, use_container_width=True)\n",
    "    report_data['cluster_scatter_data'] = (df, x_axis, y_axis)\n",
    "    \n",
    "    st.subheader(\"Cluster Centers\")\n",
    "    st.dataframe(df.groupby('Cluster').mean())\n",
    "    st.session_state.report_data = report_data\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if st.button(\"Go Back\"):\n",
    "            st.session_state.page = 'clustering'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        if st.button(\"Proceed to Action & Export\"):\n",
    "            st.session_state.page = 'action_export'\n",
    "            st.rerun()\n",
    "\n",
    "def create_pdf_report(report_data={}):\n",
    "    pdf = PDF()\n",
    "    pdf.alias_nb_pages()\n",
    "    \n",
    "    # --- Title Page ---\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", 'B', 24)\n",
    "    pdf.cell(0, 100, \"AI Data Science Pipeline Report\", 0, 1, 'C')\n",
    "    pdf.set_font(\"Arial\", 'B', 16)\n",
    "    pdf.cell(0, 10, f\"Task: {st.session_state.task}\", 0, 1, 'C')\n",
    "    pdf.ln(10)\n",
    "    pdf.set_font(\"Arial\", '', 12)\n",
    "    pdf.cell(0, 10, f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", 0, 1, 'C')\n",
    "\n",
    "    # --- Executive Summary Page ---\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", 'B', 16)\n",
    "    pdf.cell(0, 10, \"Executive Summary\", 0, 1, 'L')\n",
    "    pdf.ln(5)\n",
    "    \n",
    "    pdf.set_font(\"Arial\", '', 11)\n",
    "    selections = st.session_state.user_selections\n",
    "    task_summary = f\"The pipeline was configured to perform a {st.session_state.task} task.\"\n",
    "    if st.session_state.task == \"Predictive Modeling\":\n",
    "        task_summary += f\" The goal was to predict '{selections.get('target', 'N/A')}' using the {selections.get('algorithm', 'N/A')} algorithm.\"\n",
    "    pdf.multi_cell(0, 5, task_summary)\n",
    "    pdf.ln(10)\n",
    "    \n",
    "    if 'metrics' in st.session_state and st.session_state.metrics:\n",
    "        pdf.set_font(\"Arial\", 'B', 14)\n",
    "        pdf.cell(0, 10, \"Performance Metrics\", 0, 1, 'L')\n",
    "        pdf.set_font(\"Arial\", '', 11)\n",
    "        for key, value in st.session_state.metrics.items():\n",
    "            pdf.cell(0, 8, f\"  -  {key}: {value:.4f}\", 0, 1)\n",
    "        pdf.ln(5)\n",
    "\n",
    "    if report_data:\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", 'B', 16)\n",
    "        pdf.cell(0, 10, \"Visual Analysis\", 0, 1, 'L')\n",
    "        \n",
    "        with plt.style.context('dark_background'):\n",
    "            # --- Evaluation Charts ---\n",
    "            if 'forecast_fig' in report_data:\n",
    "                pdf.set_font(\"Arial\", 'B', 14)\n",
    "                pdf.cell(0, 10, \"Overall Forecast\", 0, 1)\n",
    "                pdf.set_font(\"Arial\", '', 11)\n",
    "                pdf.multi_cell(0, 5, \"This chart displays the historical data along with the model's forecast. The shaded area represents the uncertainty interval for the predictions.\")\n",
    "                pdf.ln(5)\n",
    "                history, forecast = report_data['forecast_fig']\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                ax.plot(history['ds'].tail(365), history['y'].tail(365), label='Historical (Last Year)')\n",
    "                ax.plot(forecast['ds'], forecast['yhat'], label='Forecast')\n",
    "                ax.fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'], color='gray', alpha=0.2)\n",
    "                plt.tight_layout()\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmpfile:\n",
    "                    fig.savefig(tmpfile.name)\n",
    "                    pdf.image(tmpfile.name, w=180)\n",
    "                os.remove(tmpfile.name)\n",
    "                plt.close(fig)\n",
    "                pdf.ln(5)\n",
    "            \n",
    "            if 'components_fig' in report_data:\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", 'B', 14)\n",
    "                pdf.cell(0, 10, \"Forecast Components\", 0, 1)\n",
    "                pdf.set_font(\"Arial\", '', 11)\n",
    "                pdf.multi_cell(0, 5, \"This plot shows the forecast components: the overall trend, and the weekly and yearly seasonal patterns discovered in the data. This helps to understand the underlying drivers of the forecast.\")\n",
    "                pdf.ln(5)\n",
    "                model, forecast = report_data['components_fig']\n",
    "                fig = prophet_plot_components(model, forecast)\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmpfile:\n",
    "                    fig.savefig(tmpfile.name)\n",
    "                    pdf.image(tmpfile.name, w=180)\n",
    "                os.remove(tmpfile.name)\n",
    "                plt.close(fig)\n",
    "                pdf.ln(5)\n",
    "            \n",
    "            if 'segment_summary_bar' in report_data:\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", 'B', 14)\n",
    "                pdf.cell(0, 10, \"Forecast Summary by Segment\", 0, 1)\n",
    "                pdf.set_font(\"Arial\", '', 11)\n",
    "                pdf.multi_cell(0, 5, \"This bar chart compares the total forecasted value across different segments, providing a high-level overview of the predictions for each group.\")\n",
    "                pdf.ln(5)\n",
    "                summary_df = report_data['segment_summary_bar']\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                ax.bar(summary_df['Segment'], summary_df['Total Forecasted Value'])\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.tight_layout()\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmpfile:\n",
    "                    fig.savefig(tmpfile.name)\n",
    "                    pdf.image(tmpfile.name, w=160)\n",
    "                os.remove(tmpfile.name)\n",
    "                plt.close(fig)\n",
    "                pdf.ln(5)\n",
    "                \n",
    "            if 'segment_forecasts' in report_data and report_data['segment_forecasts']:\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", 'B', 14)\n",
    "                pdf.cell(0, 10, \"Detailed Segment Forecasts\", 0, 1)\n",
    "                for segment, (history, forecast) in report_data['segment_forecasts'].items():\n",
    "                    pdf.set_font(\"Arial\", 'B', 12)\n",
    "                    pdf.cell(0, 10, f\"Segment: {segment}\", 0, 1)\n",
    "                    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                    ax.plot(history['ds'].tail(365), history['y'].tail(365), label='Historical')\n",
    "                    ax.plot(forecast['ds'], forecast['yhat'], label='Forecast')\n",
    "                    ax.fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'], color='gray', alpha=0.2)\n",
    "                    ax.legend()\n",
    "                    plt.tight_layout()\n",
    "                    with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmpfile:\n",
    "                        fig.savefig(tmpfile.name)\n",
    "                        pdf.image(tmpfile.name, w=180)\n",
    "                    os.remove(tmpfile.name)\n",
    "                    plt.close(fig)\n",
    "                    pdf.ln(5)\n",
    "\n",
    "    return pdf.output(dest='S').encode('latin-1')\n",
    "\n",
    "def render_action_export_page():\n",
    "    # Conditionally skip this page for time-series models\n",
    "    if st.session_state.task == \"Predictive Modeling\" and st.session_state.user_selections.get('algorithm') in [\"Prophet\", \"ARIMA\"]:\n",
    "        return\n",
    "\n",
    "    st.header(\"Action & Export\")\n",
    "\n",
    "    if st.button(\"Go Back\"):\n",
    "        if st.session_state.task == \"Clustering Analysis\":\n",
    "            st.session_state.page = 'cluster_analysis'\n",
    "        else: # Predictive Modeling\n",
    "            st.session_state.page = 'evaluation'\n",
    "        st.rerun()\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    if st.session_state.task == \"Clustering Analysis\":\n",
    "        st.subheader(\"Cluster Interpretation & Naming\")\n",
    "        df = st.session_state.processed_df\n",
    "        cluster_means = df.groupby('Cluster').mean()\n",
    "        population_means = df.drop(columns=['Cluster']).mean()\n",
    "\n",
    "        if not st.session_state.cluster_labels:\n",
    "            for cluster_id in df['Cluster'].unique():\n",
    "                st.session_state.cluster_labels[cluster_id] = f\"Cluster {cluster_id}\"\n",
    "        \n",
    "        col1, col2 = st.columns([1,2])\n",
    "        with col1:\n",
    "            st.write(\"#### Name Your Clusters\")\n",
    "            for cluster_id in sorted(df['Cluster'].unique()):\n",
    "                 st.session_state.cluster_labels[cluster_id] = st.text_input(\n",
    "                     f\"Label for Cluster {cluster_id}:\", \n",
    "                     value=st.session_state.cluster_labels[cluster_id]\n",
    "                 )\n",
    "        with col2:\n",
    "            st.write(\"#### Explore a Cluster\")\n",
    "            selected_cluster = st.selectbox(\"Select a cluster to profile:\", sorted(df['Cluster'].unique()))\n",
    "            \n",
    "            cluster_data = cluster_means.loc[selected_cluster]\n",
    "            comparison_df = pd.DataFrame({'Cluster Average': cluster_data, 'Population Average': population_means}).reset_index()\n",
    "            \n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Bar(x=comparison_df['index'], y=comparison_df['Cluster Average'], name='Cluster Average'))\n",
    "            fig.add_trace(go.Bar(x=comparison_df['index'], y=comparison_df['Population Average'], name='Population Average'))\n",
    "            fig.update_layout(barmode='group', title=f\"Profile of Cluster {selected_cluster}\", template='plotly_dark')\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        st.markdown(\"---\")\n",
    "        st.subheader(\"Export Artifacts for Clustering\")\n",
    "        labeled_df = st.session_state.original_processed_df.copy()\n",
    "        labeled_df['Cluster ID'] = st.session_state.processed_df['Cluster']\n",
    "        labeled_df['Cluster Label'] = labeled_df['Cluster ID'].map(st.session_state.cluster_labels)\n",
    "        export_csv = labeled_df.to_csv().encode('utf-8')\n",
    "        st.download_button(\"Download Labeled Data (CSV)\", export_csv, \"labeled_cluster_data.csv\", \"text/csv\")\n",
    "        if st.session_state.model:\n",
    "            model_pkl = pickle.dumps(st.session_state.model)\n",
    "            st.download_button(\"Download Trained Model (PKL)\", model_pkl, \"model.pkl\")\n",
    "        \n",
    "        st.download_button(\"Download Full Report (PDF)\", create_pdf_report(st.session_state.get('report_data', {})), \"report.pdf\")\n",
    "\n",
    "\n",
    "    else: # Predictive Modeling\n",
    "        st.subheader(\"Live Prediction Simulator\")\n",
    "        if st.session_state.model and not (st.session_state.user_selections.get('algorithm') in [\"Prophet\", \"ARIMA\"]):\n",
    "            model = st.session_state.model\n",
    "            X_test = st.session_state.X_test\n",
    "            y_test = st.session_state.y_test\n",
    "            \n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importances = model.feature_importances_\n",
    "            elif hasattr(model, 'coef_'):\n",
    "                importances = np.abs(model.coef_.flatten())\n",
    "            else:\n",
    "                importances = np.zeros(len(X_test.columns))\n",
    "\n",
    "            feature_importance_df = pd.DataFrame({'feature': X_test.columns, 'importance': importances}).sort_values('importance', ascending=False)\n",
    "            \n",
    "            base_features_df = feature_importance_df[~feature_importance_df['feature'].str.contains('_lag_|_roll_')]\n",
    "            top_features = base_features_df.head(5)['feature'].tolist()\n",
    "\n",
    "            input_data = {}\n",
    "            st.info(\"Adjust the top controllable features to see how they affect the prediction.\")\n",
    "            for feature in top_features:\n",
    "                if pd.api.types.is_numeric_dtype(X_test[feature].dtype):\n",
    "                    min_val, max_val = float(X_test[feature].min()), float(X_test[feature].max())\n",
    "                    input_data[feature] = st.slider(f\"Adjust {feature}\", min_val, max_val, float(X_test[feature].mean()))\n",
    "                else: \n",
    "                    unique_vals = list(X_test[feature].unique())\n",
    "                    input_data[feature] = st.selectbox(f\"Select {feature}\", options=unique_vals, index=0)\n",
    "\n",
    "            for col in X_test.columns:\n",
    "                if col not in input_data:\n",
    "                    input_data[col] = X_test[col].mean() if pd.api.types.is_numeric_dtype(X_test[col].dtype) else X_test[col].mode()[0]\n",
    "            \n",
    "            input_df = pd.DataFrame([input_data])[X_test.columns] \n",
    "            \n",
    "            prediction = model.predict(input_df)\n",
    "            pred_proba = model.predict_proba(input_df) if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "            if st.session_state.problem_type == \"Classification\":\n",
    "                st.success(f\"**Predicted Outcome:** `{prediction[0]}`\")\n",
    "                if pred_proba is not None:\n",
    "                    prob_df = pd.DataFrame(pred_proba, columns=model.classes_).T\n",
    "                    prob_df.columns = [\"Probability\"]\n",
    "                    st.bar_chart(prob_df)\n",
    "            else:\n",
    "                st.success(f\"**Predicted Value:** `{prediction[0]:,.2f}`\")\n",
    "                fig = go.Figure(go.Indicator(\n",
    "                    mode = \"gauge+number\",\n",
    "                    value = prediction[0],\n",
    "                    domain = {'x': [0, 1], 'y': [0, 1]},\n",
    "                    title = {'text': \"Predicted Value Gauge\"},\n",
    "                    gauge = {'axis': {'range': [y_test.min(), y_test.max()]}}\n",
    "                ))\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "# --- Main App Logic ---\n",
    "def main():\n",
    "    apply_custom_theme()\n",
    "    initialize_session_state()\n",
    "    render_sidebar()\n",
    "    page = st.session_state.page\n",
    "    \n",
    "    if page == 'upload': render_upload_page()\n",
    "    elif page == 'task_hub': render_task_hub()\n",
    "    elif page == 'preprocessing': render_preprocessing_page()\n",
    "    elif page == 'eda': render_eda_page()\n",
    "    elif page == 'action_export': render_action_export_page()\n",
    "    elif st.session_state.task == \"Predictive Modeling\":\n",
    "        if page == 'feature_engineering': render_feature_engineering_page()\n",
    "        elif page == 'model_recommendation': render_recommendation_page()\n",
    "        elif page == 'modeling': render_modeling_page()\n",
    "        elif page == 'evaluation': render_evaluation_page()\n",
    "    elif st.session_state.task == \"Clustering Analysis\":\n",
    "        if page == 'clustering': render_clustering_page()\n",
    "        elif page == 'cluster_analysis': render_cluster_analysis_page()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3b3c667-ac01-4404-b310-a4c8f4c5a110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app5.py\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from fpdf import FPDF\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Time Series Model Imports ---\n",
    "# Note: You may need to install these libraries: pip install statsmodels prophet fpdf\n",
    "try:\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    from prophet import Prophet\n",
    "    from prophet.plot import plot_plotly, plot_components_plotly\n",
    "    from prophet.plot import plot_components as prophet_plot_components\n",
    "    PROPHET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PROPHET_AVAILABLE = False\n",
    "\n",
    "# --- Advanced Model Imports ---\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LGBM_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    CATBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CATBOOST_AVAILABLE = False\n",
    "\n",
    "\n",
    "# --- Page Configuration ---\n",
    "st.set_page_config(\n",
    "    page_title=\"AI Data Science Pipeline\",\n",
    "    page_icon=\"ï¿½\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# --- Custom PDF Class with Footer ---\n",
    "class PDF(FPDF):\n",
    "    def footer(self):\n",
    "        self.set_y(-15)\n",
    "        self.set_font('Arial', 'I', 8)\n",
    "        # Page number\n",
    "        self.cell(0, 10, 'Page ' + str(self.page_no()) + '/{nb}', 0, 0, 'C')\n",
    "        # Generation date\n",
    "        self.set_x(10)\n",
    "        self.cell(0, 10, f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", 0, 0, 'L')\n",
    "\n",
    "\n",
    "# --- Custom CSS for Black Theme ---\n",
    "def apply_custom_theme():\n",
    "    \"\"\"Applies a dark, clean theme to the Streamlit app.\"\"\"\n",
    "    st.markdown(\"\"\"\n",
    "    <style>\n",
    "        /* Main background color */\n",
    "        .stApp {\n",
    "            background-color: #0E1117;\n",
    "            color: #FAFAFA;\n",
    "        }\n",
    "\n",
    "        /* Sidebar styling */\n",
    "        .st-emotion-cache-16txtl3 {\n",
    "            background-color: #1F222B;\n",
    "        }\n",
    "        \n",
    "        /* Text color in sidebar */\n",
    "        .st-emotion-cache-16txtl3 h1, .st-emotion-cache-16txtl3 h2, .st-emotion-cache-16txtl3 h3, .st-emotion-cache-16txtl3 .st-emotion-cache-1v0mbdj p {\n",
    "            color: #FAFAFA;\n",
    "        }\n",
    "\n",
    "        /* Button styling */\n",
    "        .stButton>button {\n",
    "            border: 2px solid #8A2BE2; /* Purple */\n",
    "            background-color: #8A2BE2; /* Purple */\n",
    "            color: white;\n",
    "            padding: 10px 24px;\n",
    "            text-align: center;\n",
    "            text-decoration: none;\n",
    "            display: inline-block;\n",
    "            font-size: 16px;\n",
    "            margin: 4px 2px;\n",
    "            cursor: pointer;\n",
    "            border-radius: 0.5rem;\n",
    "            transition-duration: 0.4s;\n",
    "            width: 100%;\n",
    "        }\n",
    "        .stButton>button:hover {\n",
    "            background-color: #6B21A8; /* Darker Purple */\n",
    "            border-color: #6B21A8;\n",
    "        }\n",
    "        \n",
    "        /* Header and subheader styling */\n",
    "        h1, h2, h3 {\n",
    "            color: #8A2BE2; /* Purple accent color */\n",
    "        }\n",
    "        \n",
    "        /* Dataframe styling */\n",
    "        .stDataFrame {\n",
    "            border: 1px solid #8A2BE2; /* Purple */\n",
    "            border-radius: 8px;\n",
    "        }\n",
    "\n",
    "        /* Custom Tab Styling */\n",
    "        button[data-baseweb=\"tab\"] {\n",
    "            font-size: 16px !important;\n",
    "            padding-top: 12px !important;\n",
    "            padding-bottom: 12px !important;\n",
    "        }\n",
    "        button[data-baseweb=\"tab\"] > div {\n",
    "            gap: 8px !important;\n",
    "        }\n",
    "\n",
    "    </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "@st.cache_data\n",
    "def get_column_types(df):\n",
    "    \"\"\"Identifies and categorizes columns in a DataFrame.\"\"\"\n",
    "    numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    date_cols = df.select_dtypes(include=['datetime', 'datetimetz']).columns.tolist()\n",
    "    \n",
    "    categorical_cols = [col for col in categorical_cols if col not in date_cols]\n",
    "    \n",
    "    id_cols = []\n",
    "    for col in numerical_cols:\n",
    "        if df[col].nunique() == len(df):\n",
    "            id_cols.append(col)\n",
    "    \n",
    "    numerical_cols = [col for col in numerical_cols if col not in id_cols]\n",
    "    return numerical_cols, categorical_cols, date_cols, id_cols\n",
    "\n",
    "# --- Initialize Session State ---\n",
    "def initialize_session_state():\n",
    "    \"\"\"Initializes all required keys in Streamlit's session state.\"\"\"\n",
    "    if 'page' not in st.session_state:\n",
    "        st.session_state.page = 'upload'\n",
    "    if 'task' not in st.session_state:\n",
    "        st.session_state.task = None\n",
    "    \n",
    "    if 'df' not in st.session_state: st.session_state.df = None\n",
    "    if 'processed_df' not in st.session_state: st.session_state.processed_df = None\n",
    "    if 'model' not in st.session_state: st.session_state.model = None\n",
    "    if 'forecast_df' not in st.session_state: st.session_state.forecast_df = None\n",
    "\n",
    "    if 'user_selections' not in st.session_state:\n",
    "        st.session_state.user_selections = {}\n",
    "    if 'best_params' not in st.session_state:\n",
    "        st.session_state.best_params = None\n",
    "    if 'cluster_labels' not in st.session_state:\n",
    "        st.session_state.cluster_labels = {}\n",
    "    if 'preprocessing_config' not in st.session_state:\n",
    "        st.session_state.preprocessing_config = {}\n",
    "    if 'showdown_results' not in st.session_state:\n",
    "        st.session_state.showdown_results = None\n",
    "    if 'date_converted' not in st.session_state:\n",
    "        st.session_state.date_converted = False\n",
    "    if 'report_data' not in st.session_state:\n",
    "        st.session_state.report_data = {}\n",
    "\n",
    "\n",
    "# --- Sidebar Navigation ---\n",
    "def render_sidebar():\n",
    "    \"\"\"Renders the sidebar with progress indicators based on the selected task.\"\"\"\n",
    "    \n",
    "    # --- Icon Definitions (Base64 Encoded SVGs) ---\n",
    "    chart_svg_base64 = \"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiM4QTJCRTIiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48bGluZSB4MT0iMTIiIHkxPSIyMCIgeDI9IjEyIiB5Mj0iMTAiPjwvbGluZT48bGluZSB4MT0iMTgiIHkxPSIyMCIgeDI9IjE4IiB5Mj0iNCI+PC9saW5lPjxsaW5lIHgxPSI2IiB5MT0iMjAiIHgyPSI2IiB5Mj0iMTYiPjwvbGluZT48L3N2Zz4=\"\n",
    "    check_svg_base64 = \"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiM0YWRlODAiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJNMjIgMTEuMDhWMTJhMTAgMTAgMCAxIDEtNS45My05LjE0Ii8+PHBvbHlsaW5lIHBvaW50cz0iMjIgNCAxMiAxNC4wMSA5IDExLjAxIi8+PC9zdmc+\"\n",
    "    cog_svg_base64 = \"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiM4QTJCRTIiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIiBjbGFzcz0iZmVhdGhlciBmZWF0aGVyLXNldHRpbmdzIj48Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIzIj48L2NpcmNsZT48cGF0aCBkPSJNMTkuNCAxNWExLjY1IDEuNjUgMCAwIDAgLjMzIDEuODJsLjA2LjA2YTIgMiAwIDAgMSAwIDIuODMgMiAyIDAgMCAxLTIuODMgMGwtLjA2LS4wNmExLjY1IDEuNjUgMCAwIDAtMS44Mi0uMzMgMS42NSAxLjY1IDAgMCAwLTEgMS41MVYyMWExIDIgMCAwIDEtMiAyIDIgMiAwIDAgMS0yLTJ2LS4wOUExLjY1IDEuNjUgMCAwIDAgOSAxOS40YTEuNjUgMS42NSAwIDAgMC0xLjgyLjMzbC0uMDYuMDZhMiAyIDAgMCAxLTIuODMgMCAyIDIgMCAwIDEgMC0yLjgzbC4wNi0uMDZhMS42NSAxLjY1IDEuNjUgMCAwIDAgLjMzLTEuODIgMS42NSAxLjY1IDAgMCAwLTEuNTEtMUgzYTIgMiAwIDAgMS0yLTIgMiAyIDAgMCAxIDItMmguMDlBMS42NSAxLjY1IDAgMCAwIDQuNiA5YTEuNjUgMS42NSAwIDAgMC0uMzMtMS44MmwtLjA2LS4wNmEyIDIgMCAwIDEgMC0yLjgzIDIgMiAwIDAgMSAyLjgzIDBsLjA2LjA2YTEuNjUgMS42NSAwIDAgMCAxLjgyLjMzSDlhMS42NSAxLjY1IDAgMCAwIDEtMS41MVYzYTIgMiAwIDAgMSAyLTIgMiAyIDAgMCAxIDIgMnYuMDlhMS42NSAxLjY1IDAgMCAwIDEgMS41MSAxLjY1IDEuNjUgMCAwIDAgMS44Mi0uMzNsLjA2LS4wNmEyIDIgMCAwIDEgMi44MyAwIDIgMiAwIDAgMSAwIDIuODNsLS4wNi4wNmExLjY1IDEuNjUgMCAwIDAtLjMzIDEuODJWOWExLjY1IDEuNjUgMCAwIDAgMS41MSAxSDIxYTIgMiAwIDAgMSAyIDIgMiAyIDAgMCAxLTIgMmgtLjA5YTEuNjUgMS42NSAwIDAgMC0xLjUxIDF6Ij48L3BhdGg+PC9zdmc+\"\n",
    "    square_svg_base64 = \"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiM5NGExYjgiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cmVjdCB4PSIzIiB5PSIzIiB3aWR0aD0iMTgiIGhlaWdodD0iMTgiIHJ4PSIyIiByeT0iMiIvPjwvc3ZnPg==\"\n",
    "\n",
    "    title_html = f'<div style=\"display: flex; align-items: center; gap: 15px; margin-bottom: 20px;\"><img src=\"{chart_svg_base64}\" width=\"24\" height=\"24\"> <span style=\"font-size: 1.35rem; font-weight: bold;\">Pipeline Progress</span></div>'\n",
    "    st.sidebar.write(title_html, unsafe_allow_html=True)\n",
    "    \n",
    "    task = st.session_state.get('task')\n",
    "    \n",
    "    if not task:\n",
    "        steps = ['Upload', 'Task Selection']\n",
    "    elif task == \"Predictive Modeling\":\n",
    "        algorithm = st.session_state.get('user_selections', {}).get('algorithm')\n",
    "        if algorithm in [\"Prophet\", \"ARIMA\"]:\n",
    "            steps = ['Upload', 'Task Selection', 'Preprocessing', 'EDA', 'Feature Engineering', 'Model Recommendation', 'Modeling', 'Evaluation']\n",
    "        else:\n",
    "            steps = ['Upload', 'Task Selection', 'Preprocessing', 'EDA', 'Feature Engineering', 'Model Recommendation', 'Modeling', 'Evaluation', 'Action & Export']\n",
    "    elif task == \"Clustering Analysis\":\n",
    "        steps = ['Upload', 'Task Selection', 'Preprocessing', 'EDA', 'Clustering', 'Cluster Analysis', 'Action & Export']\n",
    "    else:\n",
    "        steps = ['Upload']\n",
    "\n",
    "    page_map = {\n",
    "        'upload': 0, 'task_hub': 1, 'preprocessing': 2, 'eda': 3,\n",
    "        'feature_engineering': 4, 'model_recommendation': 5, 'modeling': 6, 'evaluation': 7,\n",
    "        'clustering': 4, 'cluster_analysis': 5, 'action_export': 8\n",
    "    }\n",
    "    \n",
    "    if st.session_state.page == 'action_export':\n",
    "         current_step_index = len(steps) - 1\n",
    "    else:\n",
    "        current_step_index = page_map.get(st.session_state.page, 0)\n",
    "\n",
    "    for i, step in enumerate(steps):\n",
    "        if i < current_step_index:\n",
    "            icon_html = f'<img src=\"{check_svg_base64}\" width=\"16\" height=\"16\" style=\"display:inline-block; vertical-align:middle; margin-right:8px;\">'\n",
    "            st.sidebar.write(f'{icon_html} <span style=\"color:#4ade80; font-size: 16px;\">{step}</span>', unsafe_allow_html=True)\n",
    "        elif i == current_step_index:\n",
    "            icon_html = f'<img src=\"{cog_svg_base64}\" width=\"16\" height=\"16\" style=\"display:inline-block; vertical-align:middle; margin-right:8px;\">'\n",
    "            st.sidebar.write(f'{icon_html} <strong style=\"color:white; font-size: 16px;\">{step}</strong>', unsafe_allow_html=True)\n",
    "        else:\n",
    "            icon_html = f'<img src=\"{square_svg_base64}\" width=\"16\" height=\"16\" style=\"display:inline-block; vertical-align:middle; margin-right:8px;\">'\n",
    "            st.sidebar.write(f'{icon_html} <span style=\"color:#94a3b8; font-size: 16px;\">{step}</span>', unsafe_allow_html=True)\n",
    "    \n",
    "    st.sidebar.markdown(\"<hr style='margin-top:20px; margin-bottom:20px;'>\", unsafe_allow_html=True)\n",
    "\n",
    "    if st.sidebar.button(\"Start Over\", key=\"start_over_sidebar\"):\n",
    "        for key in list(st.session_state.keys()):\n",
    "            del st.session_state[key]\n",
    "        st.rerun()\n",
    "\n",
    "# --- Page Rendering Functions ---\n",
    "\n",
    "def render_upload_page():\n",
    "    st.markdown(\"\"\"\n",
    "        <style>\n",
    "            .stApp > header {\n",
    "                background-color: transparent;\n",
    "            }\n",
    "            .upload-container {\n",
    "                display: flex;\n",
    "                flex-direction: column;\n",
    "                align-items: center;\n",
    "                justify-content: flex-start;\n",
    "                height: 100vh;\n",
    "                padding-top: 5rem;\n",
    "            }\n",
    "            .title {\n",
    "                font-size: 2rem; \n",
    "                font-weight: 700; \n",
    "                color: #E2E8F0; \n",
    "                letter-spacing: 0.1em;\n",
    "                margin-bottom: 4rem;\n",
    "            }\n",
    "            .upload-section {\n",
    "                width: 100%;\n",
    "                max-width: 896px; \n",
    "            }\n",
    "            .upload-header {\n",
    "                font-size: 3rem; \n",
    "                font-weight: bold;\n",
    "                color: #8A2BE2;\n",
    "                margin-bottom: 2rem;\n",
    "            }\n",
    "            .stFileUploader > div > div {\n",
    "                border: 2px dashed #4A5568;\n",
    "                background-color: rgba(45, 55, 72, 0.5);\n",
    "                padding: 3rem; \n",
    "                border-radius: 0.75rem;\n",
    "            }\n",
    "            .stFileUploader > div > div > button {\n",
    "                background-color: #8A2BE2;\n",
    "                color: white;\n",
    "                padding: 0.75rem 1.5rem;\n",
    "                font-size: 1rem;\n",
    "            }\n",
    "            .stFileUploader > div > div > button:hover {\n",
    "                background-color: #6B21A8;\n",
    "            }\n",
    "        </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "    st.markdown('<div class=\"title\" style=\"text-align: center;\">DATA SCIENTIST AI</div>', unsafe_allow_html=True)\n",
    "    \n",
    "    _ , col2, _ = st.columns([1, 3, 1]) \n",
    "    with col2:\n",
    "        st.markdown('<div class=\"upload-section\">', unsafe_allow_html=True)\n",
    "        st.markdown('<div class=\"upload-header\">Upload Data</div>', unsafe_allow_html=True)\n",
    "        \n",
    "        uploaded_file = st.file_uploader(\n",
    "            \"Drag and drop file here\", \n",
    "            type=\"csv\",\n",
    "            label_visibility=\"collapsed\"\n",
    "        )\n",
    "        \n",
    "        st.markdown('</div>', unsafe_allow_html=True)\n",
    "\n",
    "    if uploaded_file:\n",
    "        try:\n",
    "            st.session_state.df = pd.read_csv(uploaded_file)\n",
    "            st.session_state.page = 'task_hub'\n",
    "            st.rerun()\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error loading file: {e}\")\n",
    "\n",
    "def render_task_hub():\n",
    "    st.header(\"Stage 2: Task Selection\")\n",
    "    st.write(\"Your data is loaded. What would you like to do next?\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        st.subheader(\"Predictive Modeling\")\n",
    "        st.info(\"Forecast future values, classify outcomes, or predict numerical targets.\")\n",
    "        if st.button(\"Start Predictive Modeling\"):\n",
    "            st.session_state.task = \"Predictive Modeling\"\n",
    "            df = st.session_state.df.copy()\n",
    "            # Automatic date conversion runs only once here\n",
    "            for col in df.columns:\n",
    "                if col.lower() == 'date':\n",
    "                    try:\n",
    "                        df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)\n",
    "                        st.toast(f\"Automatically converted column '{col}' to datetime.\", icon=\"ð\")\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            st.session_state.processed_df = df\n",
    "            st.session_state.page = 'preprocessing'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        st.subheader(\"Clustering Analysis\")\n",
    "        st.info(\"Automatically discover hidden groups or segments in your data.\")\n",
    "        if st.button(\"Start Clustering Analysis\"):\n",
    "            st.session_state.task = \"Clustering Analysis\"\n",
    "            st.session_state.processed_df = st.session_state.df.copy()\n",
    "            st.session_state.page = 'preprocessing'\n",
    "            st.rerun()\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    if st.button(\"Go Back to Upload\"):\n",
    "        st.session_state.page = 'upload'\n",
    "        st.session_state.df = None # Allow re-upload\n",
    "        st.rerun()\n",
    "\n",
    "def render_preprocessing_page():\n",
    "    st.header(\"Configure Cleaning & Feature Engineering\")\n",
    "    df = st.session_state.processed_df.copy()\n",
    "\n",
    "    # --- NEW: Data Quality Section ---\n",
    "    st.subheader(\"Data Quality Check\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        missing_values = df.isnull().sum()\n",
    "        missing_values = missing_values[missing_values > 0]\n",
    "        if not missing_values.empty:\n",
    "            st.write(\"Columns with Missing Values:\")\n",
    "            st.dataframe(missing_values.to_frame(name='Missing Count'))\n",
    "        else:\n",
    "            st.success(\"No missing values found.\")\n",
    "    \n",
    "    with col2:\n",
    "        num_duplicates = df.duplicated().sum()\n",
    "        st.metric(\"Duplicate Rows Found\", num_duplicates)\n",
    "        if num_duplicates > 0:\n",
    "            primary_key_cols = st.multiselect(\"Select Primary Key column(s) to remove duplicates:\", df.columns)\n",
    "        else:\n",
    "            primary_key_cols = None\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "\n",
    "    # Initialize config if not present or columns have changed\n",
    "    if not st.session_state.preprocessing_config or set(st.session_state.preprocessing_config.keys()) != set(df.columns):\n",
    "        config = {}\n",
    "        numerical_cols, categorical_cols, date_cols, _ = get_column_types(df)\n",
    "        for col in df.columns:\n",
    "            col_type = 'Date' if col in date_cols else ('Numerical' if col in numerical_cols else 'Categorical')\n",
    "            config[col] = {\n",
    "                'type': col_type,\n",
    "                'missing': 'None',\n",
    "                'scaling': 'None' if col_type == 'Numerical' else 'N/A',\n",
    "                'remove': False\n",
    "            }\n",
    "        st.session_state.preprocessing_config = config\n",
    "    \n",
    "    config = st.session_state.preprocessing_config\n",
    "    \n",
    "    header_cols = st.columns([3, 2, 2, 2, 1])\n",
    "    headers = [\"COLUMN\", \"TYPE\", \"MISSING\", \"SCALING\", \"REMOVE\"]\n",
    "    for col, header in zip(header_cols, headers):\n",
    "        col.write(f\"**{header}**\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        row_cols = st.columns([3, 2, 2, 2, 1])\n",
    "        row_cols[0].write(col_name)\n",
    "        row_cols[1].info(config[col_name]['type'])\n",
    "        \n",
    "        missing_options = [\"None\", \"Mean\", \"Median\"] if config[col_name]['type'] == 'Numerical' else [\"None\", \"Mode\"]\n",
    "        if config[col_name]['type'] == 'Date': missing_options = [\"None\"]\n",
    "        config[col_name]['missing'] = row_cols[2].selectbox(\"Missing\", missing_options, key=f\"missing_{col_name}\", label_visibility=\"collapsed\")\n",
    "        \n",
    "        scaling_options = [\"None\", \"StandardScaler\", \"MinMaxScaler\"]\n",
    "        if config[col_name]['type'] == 'Numerical':\n",
    "            config[col_name]['scaling'] = row_cols[3].selectbox(\"Scaling\", scaling_options, key=f\"scaling_{col_name}\", label_visibility=\"collapsed\")\n",
    "        else:\n",
    "            row_cols[3].write(\"N/A\")\n",
    "            \n",
    "        config[col_name]['remove'] = row_cols[4].checkbox(\"\", key=f\"remove_{col_name}\", value=config[col_name]['remove'], label_visibility=\"collapsed\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"Global Encoding Strategy\")\n",
    "    encoding_strategy = st.selectbox(\"Choose how to handle all categorical columns:\", [\"Label Encoding\", \"One-Hot Encoding\"])\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if st.button(\"Go Back\"):\n",
    "            st.session_state.page = 'task_hub'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        if st.button(\"Apply Preprocessing & Proceed\"):\n",
    "            with st.spinner(\"Processing...\"):\n",
    "                proc_df = df.copy()\n",
    "                \n",
    "                if primary_key_cols:\n",
    "                    original_rows = len(proc_df)\n",
    "                    proc_df.drop_duplicates(subset=primary_key_cols, inplace=True)\n",
    "                    st.toast(f\"Removed {original_rows - len(proc_df)} duplicates.\")\n",
    "\n",
    "                cols_to_remove = [col for col, settings in config.items() if settings['remove']]\n",
    "                proc_df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n",
    "\n",
    "                for col, settings in config.items():\n",
    "                    if col in proc_df.columns:\n",
    "                        if settings['missing'] != 'None':\n",
    "                            if settings['type'] == 'Numerical':\n",
    "                                if settings['missing'] == 'Mean': proc_df[col].fillna(proc_df[col].mean(), inplace=True)\n",
    "                                elif settings['missing'] == 'Median': proc_df[col].fillna(proc_df[col].median(), inplace=True)\n",
    "                            elif settings['type'] == 'Categorical':\n",
    "                                 if settings['missing'] == 'Mode': proc_df[col].fillna(proc_df[col].mode()[0], inplace=True)\n",
    "                \n",
    "                st.session_state.original_processed_df = proc_df.copy()\n",
    "\n",
    "                categorical_cols_to_encode = [col for col, settings in config.items() if col in proc_df.columns and settings['type'] == 'Categorical']\n",
    "                if encoding_strategy == 'Label Encoding':\n",
    "                    for col in categorical_cols_to_encode:\n",
    "                        proc_df[col] = LabelEncoder().fit_transform(proc_df[col].astype(str))\n",
    "                elif encoding_strategy == 'One-Hot Encoding':\n",
    "                    proc_df = pd.get_dummies(proc_df, columns=categorical_cols_to_encode)\n",
    "\n",
    "                for col, settings in config.items():\n",
    "                     if col in proc_df.columns and settings['type'] == 'Numerical' and settings['scaling'] != 'None':\n",
    "                        scaler = StandardScaler() if settings['scaling'] == 'StandardScaler' else MinMaxScaler()\n",
    "                        proc_df[[col]] = scaler.fit_transform(proc_df[[col]])\n",
    "                \n",
    "                proc_df.dropna(inplace=True)\n",
    "                st.session_state.processed_df = proc_df\n",
    "                st.session_state.page = 'eda'\n",
    "                st.rerun()\n",
    "\n",
    "def render_eda_page():\n",
    "    st.header(\"Exploratory Data Analysis (EDA)\")\n",
    "    df = st.session_state.original_processed_df.copy()\n",
    "    report_data = st.session_state.get('report_data', {})\n",
    "    \n",
    "    st.subheader(\"1. Univariate Analysis\")\n",
    "    col1_uni, col2_uni = st.columns([1,2])\n",
    "    with col1_uni:\n",
    "        feature_uni = st.selectbox(\"Select a feature to visualize its distribution:\", df.columns)\n",
    "    with col2_uni:\n",
    "        is_numeric_uni = pd.api.types.is_numeric_dtype(df[feature_uni].dtype)\n",
    "        if is_numeric_uni:\n",
    "            fig = px.histogram(df, x=feature_uni, title=f\"Distribution of {feature_uni}\", template='plotly_dark')\n",
    "        else:\n",
    "            fig = px.bar(df[feature_uni].value_counts(), title=f\"Count of {feature_uni}\", template='plotly_dark')\n",
    "        st.plotly_chart(fig, use_container_width=True)\n",
    "        report_data['univariate_data'] = (df[feature_uni], is_numeric_uni)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"2. Bivariate Analysis\")\n",
    "    col1_bi, col2_bi = st.columns(2)\n",
    "    with col1_bi:\n",
    "        x_axis = st.selectbox(\"Select X-Axis Feature:\", df.columns, key=\"x_axis\")\n",
    "    with col2_bi:\n",
    "        y_axis = st.selectbox(\"Select Y-Axis Feature:\", df.columns, key=\"y_axis\")\n",
    "    \n",
    "    x_is_num = pd.api.types.is_numeric_dtype(df[x_axis].dtype)\n",
    "    y_is_num = pd.api.types.is_numeric_dtype(df[y_axis].dtype)\n",
    "    \n",
    "    if x_axis != y_axis:\n",
    "        if x_is_num and y_is_num:\n",
    "            fig_bi = px.scatter(df, x=x_axis, y=y_axis, title=f\"{x_axis} vs. {y_axis}\", template='plotly_dark')\n",
    "        elif (x_is_num and not y_is_num):\n",
    "            fig_bi = px.box(df, x=y_axis, y=x_axis, title=f\"{y_axis} vs. {x_axis}\", template='plotly_dark')\n",
    "        elif (not x_is_num and y_is_num):\n",
    "            fig_bi = px.box(df, x=x_axis, y=y_axis, title=f\"{x_axis} vs. {y_axis}\", template='plotly_dark')\n",
    "        else:\n",
    "            pivot = df.groupby([x_axis, y_axis]).size().reset_index(name='count')\n",
    "            fig_bi = px.bar(pivot, x=x_axis, y='count', color=y_axis, barmode='group', title=f\"{x_axis} vs. {y_axis}\", template='plotly_dark')\n",
    "        st.plotly_chart(fig_bi, use_container_width=True)\n",
    "        report_data['bivariate_data'] = (df, x_axis, y_axis)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"3. Correlation Heatmap\")\n",
    "    num_df = df.select_dtypes(include=np.number)\n",
    "    if not num_df.empty:\n",
    "        corr = num_df.corr()\n",
    "        fig_corr = px.imshow(corr, text_auto=True, aspect=\"auto\", title=\"Correlation Matrix of Numerical Features\", template='plotly_dark')\n",
    "        st.plotly_chart(fig_corr, use_container_width=True)\n",
    "        report_data['correlation_data'] = corr\n",
    "    \n",
    "    st.session_state.report_data = report_data\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if st.button(\"Go Back\"):\n",
    "            st.session_state.page = 'preprocessing'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        if st.button(\"Proceed\"):\n",
    "            if st.session_state.task == \"Predictive Modeling\":\n",
    "                st.session_state.page = 'feature_engineering'\n",
    "            else:\n",
    "                st.session_state.page = 'clustering'\n",
    "            st.rerun()\n",
    "\n",
    "def render_feature_engineering_page():\n",
    "    st.header(\"Feature Engineering (for Predictive Modeling)\")\n",
    "    df = st.session_state.processed_df\n",
    "    st.write(\"#### Current Data Preview\"); st.dataframe(df.head())\n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    _, categorical_cols, date_cols, _ = get_column_types(st.session_state.original_processed_df)\n",
    "    \n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        st.subheader(\"Time-Series Features\")\n",
    "        if date_cols:\n",
    "            time_index = st.selectbox(\"Select Time Index Column:\", [\"None\"] + date_cols)\n",
    "            extract_parts = st.multiselect(\"Extract Date Parts:\", ['Year', 'Quarter', 'Month', 'Week', 'Day', 'Dayofweek']) if time_index != \"None\" else []\n",
    "            segment_by = st.selectbox(\"Segment Forecast By (Optional):\", [\"None\"] + categorical_cols)\n",
    "        else:\n",
    "            st.info(\"No date columns were detected in the preprocessed data.\")\n",
    "            time_index, extract_parts, segment_by = \"None\", [], \"None\"\n",
    "    with col2:\n",
    "        st.subheader(\"Lag & Rolling Features\")\n",
    "        num_cols = [c for c in df.select_dtypes(include=np.number).columns]\n",
    "        feature_col = st.selectbox(\"Select column for Lag/Rolling:\", [\"None\"] + num_cols)\n",
    "        if feature_col != \"None\":\n",
    "            lag_periods = st.text_input(\"Lag Periods (comma-separated)\", \"1, 7\")\n",
    "            rolling_window = st.number_input(\"Rolling Window Size\", min_value=0, value=7)\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"Feature Selection\")\n",
    "    \n",
    "    col_fs1, col_fs2 = st.columns(2)\n",
    "    with col_fs1:\n",
    "        st.write(\"#### Manual Selection\")\n",
    "        manual_features = st.multiselect(\"Manually select features to keep:\", df.columns, help=\"These features will always be included.\")\n",
    "    \n",
    "    with col_fs2:\n",
    "        st.write(\"#### Automated Selection\")\n",
    "        enable_rfe = st.checkbox(\"Enable Automated Feature Selection (RFE)\")\n",
    "        if enable_rfe:\n",
    "            num_features_to_select = st.slider(\"Number of top features to select:\", 1, len(df.columns)-1, min(10, len(df.columns)-1))\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if st.button(\"Go Back\"):\n",
    "            st.session_state.page = 'eda'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        if st.button(\"Apply Features & Proceed\"):\n",
    "            with st.spinner(\"Engineering and Selecting Features...\"):\n",
    "                proc_df = df.copy()\n",
    "                original_df = st.session_state.original_processed_df.copy()\n",
    "                st.session_state.user_selections.update({\n",
    "                    'time_index': time_index, 'date_parts': extract_parts, 'segment_by': segment_by,\n",
    "                    'feature_eng_col': feature_col,\n",
    "                    'lag_periods': lag_periods if feature_col != \"None\" else \"\", 'rolling_window': rolling_window if feature_col != \"None\" else 0,\n",
    "                    'manual_features': manual_features,\n",
    "                    'rfe_enabled': enable_rfe, 'rfe_features': num_features_to_select if enable_rfe else 0\n",
    "                })\n",
    "                if time_index != \"None\" and time_index in original_df.columns:\n",
    "                    proc_df[time_index] = pd.to_datetime(original_df[time_index])\n",
    "                    if 'Year' in extract_parts: proc_df['Year'] = proc_df[time_index].dt.year\n",
    "                    if 'Quarter' in extract_parts: proc_df['Quarter'] = proc_df[time_index].dt.quarter\n",
    "                    if 'Month' in extract_parts: proc_df['Month'] = proc_df[time_index].dt.month\n",
    "                    proc_df.drop(columns=[time_index], inplace=True, errors='ignore')\n",
    "\n",
    "                if feature_col != \"None\":\n",
    "                    if lag_periods:\n",
    "                        periods = [int(p.strip()) for p in lag_periods.split(',') if p.strip()]\n",
    "                        for p in periods:\n",
    "                            proc_df[f'{feature_col}_lag_{p}'] = proc_df[feature_col].shift(p)\n",
    "                    if rolling_window > 0:\n",
    "                        proc_df[f'{feature_col}_roll_mean_{rolling_window}'] = proc_df[feature_col].rolling(window=rolling_window).mean()\n",
    "                \n",
    "                proc_df.dropna(inplace=True)\n",
    "                st.session_state.processed_df = proc_df\n",
    "                st.session_state.page = 'model_recommendation'\n",
    "                st.rerun()\n",
    "\n",
    "def render_recommendation_page():\n",
    "    st.header(\"Model Recommendation\")\n",
    "    df = st.session_state.processed_df\n",
    "    selections = st.session_state.user_selections\n",
    "    \n",
    "    is_time_series = selections.get('time_index') != 'None'\n",
    "    \n",
    "    if is_time_series:\n",
    "        st.success(\"**Top Recommendation: Prophet**\")\n",
    "        st.info(\"Because you selected a time index, your goal is likely forecasting. Prophet is a specialized forecasting model that is robust to missing data and shifts in trend, and typically handles seasonality well.\")\n",
    "        st.warning(\"**Secondary Option: ARIMA**\")\n",
    "        st.info(\"ARIMA is another powerful statistical model for time-series data.\")\n",
    "    else:\n",
    "        st.info(\"This is a data-driven model showdown. We train several models on a sample of your data to see which performs best, giving you an intelligent starting point.\")\n",
    "        target_column = st.selectbox(\"Select your target column for the showdown:\", df.columns, index=len(df.columns)-1)\n",
    "\n",
    "        if st.button(\"Find Best Model for My Data\"):\n",
    "            with st.spinner(\"Running model showdown... This may take a moment.\"):\n",
    "                problem_type = \"Regression\" if pd.api.types.is_numeric_dtype(df[target_column].dtype) else \"Classification\"\n",
    "                \n",
    "                sample_df = df.select_dtypes(exclude=['datetime', 'datetimetz']).sample(n=min(len(df), 1000), random_state=42)\n",
    "                \n",
    "                for col in sample_df.select_dtypes(include=['object', 'category']).columns:\n",
    "                    sample_df[col] = LabelEncoder().fit_transform(sample_df[col].astype(str))\n",
    "\n",
    "                X = sample_df.drop(columns=[target_column])\n",
    "                y = sample_df[target_column]\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "                \n",
    "                models_to_test = {\n",
    "                    \"Linear/Logistic Regression\": LinearRegression() if problem_type == \"Regression\" else LogisticRegression(),\n",
    "                    \"Random Forest\": RandomForestRegressor(random_state=42) if problem_type == \"Regression\" else RandomForestClassifier(random_state=42),\n",
    "                }\n",
    "                if XGB_AVAILABLE:\n",
    "                    models_to_test[\"XGBoost\"] = xgb.XGBRegressor(random_state=42) if problem_type == \"Regression\" else xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "                results = []\n",
    "                for name, model in models_to_test.items():\n",
    "                    model.fit(X_train, y_train)\n",
    "                    preds = model.predict(X_test)\n",
    "                    if problem_type == \"Regression\":\n",
    "                        score = r2_score(y_test, preds)\n",
    "                        metric_name = \"R-Squared\"\n",
    "                    else:\n",
    "                        score = accuracy_score(y_test, preds)\n",
    "                        metric_name = \"Accuracy\"\n",
    "                    results.append({\"Model\": name, metric_name: score})\n",
    "                \n",
    "                results_df = pd.DataFrame(results).sort_values(by=metric_name, ascending=False).reset_index(drop=True)\n",
    "                st.session_state.showdown_results = results_df\n",
    "                \n",
    "    if st.session_state.showdown_results is not None:\n",
    "        st.subheader(\"Model Showdown Leaderboard\")\n",
    "        st.dataframe(st.session_state.showdown_results)\n",
    "        top_model = st.session_state.showdown_results.iloc[0][\"Model\"]\n",
    "        st.success(f\"**Top Recommendation:** Based on the showdown, **{top_model}** is the recommended model.\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    st.write(\"You can accept the recommendation or choose any model on the next page.\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if st.button(\"Go Back\"):\n",
    "            st.session_state.page = 'feature_engineering'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        if st.button(\"Proceed to Modeling\"):\n",
    "            st.session_state.page = 'modeling'\n",
    "            st.rerun()\n",
    "\n",
    "def render_modeling_page():\n",
    "    st.header(\"Modeling (Predictive)\")\n",
    "    df = st.session_state.processed_df\n",
    "    selections = st.session_state.user_selections\n",
    "    \n",
    "    target_column = st.selectbox(\"Select Target Column:\", df.columns, index=len(df.columns)-1, key=\"target_select\")\n",
    "    problem_type = \"Regression\" if pd.api.types.is_numeric_dtype(df[target_column].dtype) else \"Classification\"\n",
    "    st.session_state.problem_type = problem_type\n",
    "\n",
    "    if selections.get('rfe_enabled') and 'rfe_ran' not in st.session_state:\n",
    "        with st.spinner(\"Running Recursive Feature Elimination...\"):\n",
    "            temp_df = df.select_dtypes(exclude=['datetime', 'datetimetz']).dropna()\n",
    "            X_temp = temp_df.drop(columns=[target_column])\n",
    "            y_temp = temp_df[target_column]\n",
    "            estimator = RandomForestRegressor(n_estimators=50, random_state=42) if problem_type == \"Regression\" else RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "            rfe = RFE(estimator, n_features_to_select=selections['rfe_features'])\n",
    "            rfe.fit(X_temp, y_temp)\n",
    "            \n",
    "            rfe_selected_features = list(X_temp.columns[rfe.support_])\n",
    "            manual_features = selections.get('manual_features', [])\n",
    "            \n",
    "            final_features = list(set(rfe_selected_features + manual_features))\n",
    "            \n",
    "            df = df[final_features + [target_column]]\n",
    "            st.session_state.processed_df = df\n",
    "            st.session_state['rfe_ran'] = True\n",
    "            st.success(f\"RFE Complete. Final features selected: {', '.join(final_features)}\")\n",
    "\n",
    "    st.write(\"#### Final Data for Modeling\"); st.dataframe(df.head())\n",
    "    st.markdown(\"---\")\n",
    "\n",
    "    ml_algorithms = [\"Random Forest\", \"Gradient Boosting\", \"Support Vector Machine (SVM)\", \"Linear/Logistic Regression\"]\n",
    "    if XGB_AVAILABLE: ml_algorithms.append(\"XGBoost\")\n",
    "    if LGBM_AVAILABLE: ml_algorithms.append(\"LightGBM\")\n",
    "    if CATBOOST_AVAILABLE: ml_algorithms.append(\"CatBoost\")\n",
    "    ts_algorithms = [\"Prophet\", \"ARIMA\"] if PROPHET_AVAILABLE else []\n",
    "    \n",
    "    algorithm = st.selectbox(\"Select Algorithm:\", ml_algorithms + ts_algorithms, key=\"algo_select\")\n",
    "    \n",
    "    is_ts_model = algorithm in ts_algorithms\n",
    "    if is_ts_model:\n",
    "        if selections.get('time_index') == 'None':\n",
    "            st.error(\"ARIMA and Prophet require a time index column.\")\n",
    "            return\n",
    "        if problem_type == \"Classification\":\n",
    "            st.error(\"ARIMA and Prophet are for regression problems only.\")\n",
    "            return\n",
    "\n",
    "    if st.button(\"Train Model\"):\n",
    "        st.session_state.user_selections['target'] = target_column\n",
    "        st.session_state.user_selections['algorithm'] = algorithm\n",
    "        \n",
    "        with st.spinner(f\"Training {algorithm} model...\"):\n",
    "            if is_ts_model:\n",
    "                models = {}\n",
    "                # Train overall model\n",
    "                ts_df = st.session_state.df.copy()\n",
    "                time_col = selections['time_index']\n",
    "                ts_df['ds'] = pd.to_datetime(ts_df[time_col], dayfirst=True)\n",
    "                ts_df.rename(columns={target_column: 'y'}, inplace=True)\n",
    "                \n",
    "                if algorithm == \"Prophet\":\n",
    "                    model_overall = Prophet().fit(ts_df[['ds', 'y']])\n",
    "                elif algorithm == \"ARIMA\":\n",
    "                    model_overall = ARIMA(ts_df['y'].values, order=(5,1,0)).fit()\n",
    "                models['Overall'] = model_overall\n",
    "\n",
    "                # Train segmented models if applicable\n",
    "                segment_by = selections.get('segment_by')\n",
    "                if segment_by and segment_by != \"None\":\n",
    "                    for segment_value in st.session_state.df[segment_by].unique():\n",
    "                        segment_df = st.session_state.df[st.session_state.df[segment_by] == segment_value].copy()\n",
    "                        segment_df['ds'] = pd.to_datetime(segment_df[time_col], dayfirst=True)\n",
    "                        segment_df.rename(columns={target_column: 'y'}, inplace=True)\n",
    "                        if algorithm == \"Prophet\":\n",
    "                            model_segment = Prophet().fit(segment_df[['ds', 'y']])\n",
    "                        elif algorithm == \"ARIMA\":\n",
    "                            model_segment = ARIMA(segment_df['y'].values, order=(5,1,0)).fit()\n",
    "                        models[segment_value] = model_segment\n",
    "                st.session_state.model = models\n",
    "            else:\n",
    "                X = df.select_dtypes(exclude=['datetime', 'datetimetz']).drop(columns=[target_column])\n",
    "                y = df[target_column]\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                \n",
    "                model_options = {\n",
    "                    \"Random Forest\": RandomForestRegressor(random_state=42) if problem_type == \"Regression\" else RandomForestClassifier(random_state=42),\n",
    "                    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42) if problem_type == \"Regression\" else GradientBoostingClassifier(random_state=42),\n",
    "                    \"Support Vector Machine (SVM)\": SVR() if problem_type == \"Regression\" else SVC(random_state=42, probability=True),\n",
    "                    \"Linear/Logistic Regression\": LinearRegression() if problem_type == \"Regression\" else LogisticRegression(random_state=42, max_iter=1000),\n",
    "                    \"XGBoost\": xgb.XGBRegressor(random_state=42) if problem_type == \"Regression\" else xgb.XGBClassifier(random_state=42),\n",
    "                    \"LightGBM\": lgb.LGBMRegressor(random_state=42) if problem_type == \"Regression\" else lgb.LGBMClassifier(random_state=42),\n",
    "                    \"CatBoost\": cb.CatBoostRegressor(random_state=42, verbose=0) if problem_type == \"Regression\" else cb.CatBoostClassifier(random_state=42, verbose=0)\n",
    "                }\n",
    "                model = model_options[algorithm]\n",
    "                model.fit(X_train, y_train)\n",
    "                st.session_state.model = model\n",
    "                st.session_state.X_test = X_test\n",
    "                st.session_state.y_test = y_test\n",
    "                \n",
    "                st.subheader(\"Quick Test Set Performance\")\n",
    "                predictions = model.predict(X_test)\n",
    "                if problem_type == \"Regression\":\n",
    "                    plot_df = pd.DataFrame({'Actual': y_test.values, 'Predicted': predictions}).reset_index()\n",
    "                    fig_pred = px.line(plot_df, x='index', y=['Actual', 'Predicted'], title='Actual vs. Predicted on Test Set', template='plotly_dark')\n",
    "                    st.plotly_chart(fig_pred, use_container_width=True)\n",
    "            \n",
    "            st.success(f\"{algorithm} model(s) trained successfully!\")\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if st.button(\"Go Back\"):\n",
    "            st.session_state.page = 'model_recommendation'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        if st.button(\"Proceed to Full Evaluation\"):\n",
    "            st.session_state.page = 'evaluation'\n",
    "            st.rerun()\n",
    "\n",
    "def render_evaluation_page():\n",
    "    st.header(\"Evaluation & Explainability (Predictive)\")\n",
    "    model_or_models = st.session_state.model\n",
    "    selections = st.session_state.user_selections\n",
    "    algorithm = selections.get('algorithm', 'Unknown Model')\n",
    "    is_ts_model = algorithm in [\"Prophet\", \"ARIMA\"]\n",
    "    report_data = st.session_state.get('report_data', {})\n",
    "    report_data['segment_forecasts'] = {}\n",
    "\n",
    "    if is_ts_model:\n",
    "        tab1, tab2, tab3 = st.tabs([\"ð Forecast\", \"ð Components\", \"ð Automated Report\"])\n",
    "        with tab1:\n",
    "            st.subheader(\"Overall Forecast\")\n",
    "            overall_model = model_or_models['Overall']\n",
    "            with st.spinner(f\"Generating overall forecast plot...\"):\n",
    "                if algorithm == \"Prophet\":\n",
    "                    horizon = selections.get('forecast_horizon', 365)\n",
    "                    freq = selections.get('forecast_freq', 'D')\n",
    "                    future = overall_model.make_future_dataframe(periods=horizon, freq=freq)\n",
    "                    forecast = overall_model.predict(future)\n",
    "                    st.session_state.forecast_df = forecast\n",
    "                    \n",
    "                    fig = go.Figure()\n",
    "                    fig.add_trace(go.Scatter(x=overall_model.history['ds'].tail(365), y=overall_model.history['y'].tail(365), mode='lines', name='Historical (Last Year)'))\n",
    "                    fig.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat'], mode='lines', name='Forecast'))\n",
    "                    fig.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat_upper'], fill=None, mode='lines', line_color='rgba(0,0,0,0)'))\n",
    "                    fig.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat_lower'], fill='tonexty', mode='lines', line_color='rgba(0,0,0,0)'))\n",
    "                    st.plotly_chart(fig, use_container_width=True)\n",
    "                    report_data['forecast_fig'] = (overall_model.history, forecast)\n",
    "            \n",
    "            if len(model_or_models) > 1:\n",
    "                st.subheader(\"Forecast Summary by Segment\")\n",
    "                forecast_summaries = []\n",
    "                for segment, model in model_or_models.items():\n",
    "                    if segment != 'Overall' and algorithm == \"Prophet\":\n",
    "                        future = model.make_future_dataframe(periods=selections.get('forecast_horizon', 365), freq=selections.get('forecast_freq', 'D'))\n",
    "                        forecast = model.predict(future)\n",
    "                        total_forecast = forecast['yhat'].sum()\n",
    "                        forecast_summaries.append({'Segment': segment, 'Total Forecasted Value': total_forecast})\n",
    "                \n",
    "                if forecast_summaries:\n",
    "                    summary_df = pd.DataFrame(forecast_summaries)\n",
    "                    fig_bar = px.bar(summary_df, x='Segment', y='Total Forecasted Value', title='Total Forecasted Value by Segment', template='plotly_dark')\n",
    "                    st.plotly_chart(fig_bar, use_container_width=True)\n",
    "                    report_data['segment_summary_bar'] = summary_df\n",
    "\n",
    "                with st.expander(\"View Detailed Forecasts by Segment\"):\n",
    "                    for segment, model in model_or_models.items():\n",
    "                        if segment != 'Overall':\n",
    "                            st.write(f\"### Forecast for: {segment}\")\n",
    "                            with st.spinner(f\"Generating forecast plot for {segment}...\"):\n",
    "                                if algorithm == \"Prophet\":\n",
    "                                    future = model.make_future_dataframe(periods=selections.get('forecast_horizon', 365), freq=selections.get('forecast_freq', 'D'))\n",
    "                                    forecast = model.predict(future)\n",
    "                                    \n",
    "                                    fig_segment = go.Figure()\n",
    "                                    fig_segment.add_trace(go.Scatter(x=model.history['ds'].tail(365), y=model.history['y'].tail(365), mode='lines', name='Historical'))\n",
    "                                    fig_segment.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat'], mode='lines', name='Forecast'))\n",
    "                                    fig_segment.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat_upper'], fill=None, mode='lines', line_color='rgba(0,0,0,0)'))\n",
    "                                    fig_segment.add_trace(go.Scatter(x=forecast['ds'], y=forecast['yhat_lower'], fill='tonexty', mode='lines', line_color='rgba(0,0,0,0)'))\n",
    "                                    st.plotly_chart(fig_segment, use_container_width=True)\n",
    "                                    report_data['segment_forecasts'][segment] = (model.history, forecast)\n",
    "        with tab2:\n",
    "            st.subheader(\"Forecast Components\")\n",
    "            if algorithm == \"Prophet\":\n",
    "                fig_components = plot_components_plotly(model_or_models['Overall'], st.session_state.forecast_df)\n",
    "                st.plotly_chart(fig_components, use_container_width=True)\n",
    "                report_data['components_fig'] = (model_or_models['Overall'], st.session_state.forecast_df)\n",
    "\n",
    "        with tab3:\n",
    "            st.subheader(\"Export Artifacts\")\n",
    "            if st.session_state.forecast_df is not None:\n",
    "                forecast_csv = st.session_state.forecast_df.to_csv().encode('utf-8')\n",
    "                st.download_button(\"Download Forecast Data (CSV)\", forecast_csv, \"forecast_data.csv\", \"text/csv\")\n",
    "\n",
    "            if st.session_state.model:\n",
    "                model_pkl = pickle.dumps(st.session_state.model)\n",
    "                st.download_button(\"Download Trained Model (PKL)\", model_pkl, \"model.pkl\")\n",
    "            \n",
    "            st.download_button(\"Download Full Report (PDF)\", create_pdf_report(report_data), \"report.pdf\")\n",
    "\n",
    "    else:\n",
    "        tab1, tab2 = st.tabs([\"ð Dashboard\", \"ð Automated Report\"])\n",
    "        with tab1:\n",
    "            st.subheader(\"Performance Metrics\")\n",
    "            X_test, y_test = st.session_state.X_test, st.session_state.y_test\n",
    "            predictions = model_or_models.predict(X_test)\n",
    "            metrics = {}\n",
    "            if st.session_state.problem_type == \"Regression\":\n",
    "                metrics['R-Squared'] = r2_score(y_test, predictions)\n",
    "                metrics['MAE'] = mean_absolute_error(y_test, predictions)\n",
    "            else:\n",
    "                metrics['Accuracy'] = accuracy_score(y_test, predictions)\n",
    "                metrics['F1-Score'] = f1_score(y_test, predictions, average='weighted')\n",
    "            st.session_state.metrics = metrics\n",
    "            metric_cols = st.columns(len(metrics))\n",
    "            for i, (key, value) in enumerate(metrics.items()):\n",
    "                metric_cols[i].metric(key, f\"{value:.4f}\")\n",
    "            \n",
    "            if st.session_state.problem_type == \"Classification\":\n",
    "                st.subheader(\"ROC Curve\")\n",
    "                y_pred_proba = model_or_models.predict_proba(st.session_state.X_test)[:, 1]\n",
    "                fpr, tpr, _ = roc_curve(st.session_state.y_test, y_pred_proba)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                fig_roc = go.Figure(data=go.Scatter(x=fpr, y=tpr, mode='lines', name=f'ROC curve (area = {roc_auc:.2f})'))\n",
    "                fig_roc.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Chance', line=dict(dash='dash')))\n",
    "                st.plotly_chart(fig_roc, use_container_width=True)\n",
    "                report_data['roc_curve'] = (fpr, tpr, roc_auc)\n",
    "        \n",
    "        with tab2:\n",
    "            st.subheader(\"Export Artifacts\")\n",
    "            processed_csv = st.session_state.processed_df.to_csv().encode('utf-8')\n",
    "            st.download_button(\"Download Processed Data (CSV)\", processed_csv, \"processed_data.csv\", \"text/csv\")\n",
    "\n",
    "            if st.session_state.model:\n",
    "                model_pkl = pickle.dumps(st.session_state.model)\n",
    "                st.download_button(\"Download Trained Model (PKL)\", model_pkl, \"model.pkl\")\n",
    "            \n",
    "            st.download_button(\"Download Full Report (PDF)\", create_pdf_report(report_data), \"report.pdf\")\n",
    "    \n",
    "    st.session_state.report_data = report_data\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if st.button(\"Go Back\"):\n",
    "            st.session_state.page = 'modeling'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        if st.button(\"Proceed to Action & Export\"):\n",
    "            st.session_state.page = 'action_export'\n",
    "            st.rerun()\n",
    "\n",
    "def render_clustering_page():\n",
    "    st.header(\"Clustering Model (K-Means)\")\n",
    "    df = st.session_state.processed_df\n",
    "    st.write(\"#### Data for Clustering\"); st.dataframe(df.head())\n",
    "    \n",
    "    col1, col2 = st.columns([2,1])\n",
    "    with col1:\n",
    "        st.subheader(\"Find Optimal Number of Clusters (Elbow Method)\")\n",
    "        with st.spinner(\"Calculating inertia...\"):\n",
    "            inertia = []\n",
    "            k_range = range(1, 11)\n",
    "            for k in k_range:\n",
    "                kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "                kmeans.fit(df)\n",
    "                inertia.append(kmeans.inertia_)\n",
    "            fig = px.line(x=k_range, y=inertia, title=\"Elbow Method for Optimal k\")\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "    with col2:\n",
    "        k = st.number_input(\"Select Number of Clusters (k)\", 2, 20, 3)\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if st.button(\"Go Back\"):\n",
    "            st.session_state.page = 'eda'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        if st.button(\"Run Clustering & Analyze Results\"):\n",
    "            kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "            df['Cluster'] = kmeans.fit_predict(df)\n",
    "            st.session_state.processed_df = df\n",
    "            st.session_state.model = kmeans\n",
    "            st.session_state.page = 'cluster_analysis'\n",
    "            st.rerun()\n",
    "\n",
    "def render_cluster_analysis_page():\n",
    "    st.header(\"Cluster Analysis\")\n",
    "    df = st.session_state.processed_df\n",
    "    report_data = st.session_state.get('report_data', {})\n",
    "    \n",
    "    st.subheader(\"Cluster Population\")\n",
    "    fig_bar = px.bar(df['Cluster'].value_counts(), title=\"Cluster Population\", template='plotly_dark')\n",
    "    st.plotly_chart(fig_bar, use_container_width=True)\n",
    "    report_data['cluster_population_data'] = df['Cluster'].value_counts()\n",
    "    \n",
    "    st.subheader(\"Cluster Scatter Plot\")\n",
    "    all_cols = [col for col in df.columns if col != 'Cluster']\n",
    "    x_axis = st.selectbox(\"X-Axis\", all_cols, index=0)\n",
    "    y_axis = st.selectbox(\"Y-Axis\", all_cols, index=1)\n",
    "    fig_scatter = px.scatter(df, x=x_axis, y=y_axis, color=\"Cluster\", title=f\"{x_axis} vs. {y_axis} by Cluster\")\n",
    "    st.plotly_chart(fig_scatter, use_container_width=True)\n",
    "    report_data['cluster_scatter_data'] = (df, x_axis, y_axis)\n",
    "    \n",
    "    st.subheader(\"Cluster Centers\")\n",
    "    st.dataframe(df.groupby('Cluster').mean())\n",
    "    st.session_state.report_data = report_data\n",
    "\n",
    "    st.markdown(\"---\")\n",
    "    col1, col2 = st.columns(2)\n",
    "    with col1:\n",
    "        if st.button(\"Go Back\"):\n",
    "            st.session_state.page = 'clustering'\n",
    "            st.rerun()\n",
    "    with col2:\n",
    "        if st.button(\"Proceed to Action & Export\"):\n",
    "            st.session_state.page = 'action_export'\n",
    "            st.rerun()\n",
    "\n",
    "def create_pdf_report(report_data={}):\n",
    "    pdf = PDF()\n",
    "    pdf.alias_nb_pages()\n",
    "    \n",
    "    # --- Title Page ---\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", 'B', 24)\n",
    "    pdf.cell(0, 100, \"AI Data Science Pipeline Report\", 0, 1, 'C')\n",
    "    pdf.set_font(\"Arial\", 'B', 16)\n",
    "    pdf.cell(0, 10, f\"Task: {st.session_state.task}\", 0, 1, 'C')\n",
    "    pdf.ln(10)\n",
    "    pdf.set_font(\"Arial\", '', 12)\n",
    "    pdf.cell(0, 10, f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", 0, 1, 'C')\n",
    "\n",
    "    # --- Executive Summary Page ---\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", 'B', 16)\n",
    "    pdf.cell(0, 10, \"Executive Summary\", 0, 1, 'L')\n",
    "    pdf.ln(5)\n",
    "    \n",
    "    pdf.set_font(\"Arial\", '', 11)\n",
    "    selections = st.session_state.user_selections\n",
    "    task_summary = f\"The pipeline was configured to perform a {st.session_state.task} task.\"\n",
    "    if st.session_state.task == \"Predictive Modeling\":\n",
    "        task_summary += f\" The goal was to predict '{selections.get('target', 'N/A')}' using the {selections.get('algorithm', 'N/A')} algorithm.\"\n",
    "    pdf.multi_cell(0, 5, task_summary)\n",
    "    pdf.ln(10)\n",
    "    \n",
    "    if 'metrics' in st.session_state and st.session_state.metrics:\n",
    "        pdf.set_font(\"Arial\", 'B', 14)\n",
    "        pdf.cell(0, 10, \"Performance Metrics\", 0, 1, 'L')\n",
    "        pdf.set_font(\"Arial\", '', 11)\n",
    "        for key, value in st.session_state.metrics.items():\n",
    "            pdf.cell(0, 8, f\"  -  {key}: {value:.4f}\", 0, 1)\n",
    "        pdf.ln(5)\n",
    "\n",
    "    if report_data:\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", 'B', 16)\n",
    "        pdf.cell(0, 10, \"Visual Analysis\", 0, 1, 'L')\n",
    "        \n",
    "        with plt.style.context('dark_background'):\n",
    "            # --- Evaluation Charts ---\n",
    "            if 'forecast_fig' in report_data:\n",
    "                pdf.set_font(\"Arial\", 'B', 14)\n",
    "                pdf.cell(0, 10, \"Overall Forecast\", 0, 1)\n",
    "                pdf.set_font(\"Arial\", '', 11)\n",
    "                pdf.multi_cell(0, 5, \"This chart displays the historical data along with the model's forecast. The shaded area represents the uncertainty interval for the predictions.\")\n",
    "                pdf.ln(5)\n",
    "                history, forecast = report_data['forecast_fig']\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                ax.plot(history['ds'].tail(365), history['y'].tail(365), label='Historical (Last Year)')\n",
    "                ax.plot(forecast['ds'], forecast['yhat'], label='Forecast')\n",
    "                ax.fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'], color='gray', alpha=0.2)\n",
    "                plt.tight_layout()\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmpfile:\n",
    "                    fig.savefig(tmpfile.name)\n",
    "                    pdf.image(tmpfile.name, w=180)\n",
    "                os.remove(tmpfile.name)\n",
    "                plt.close(fig)\n",
    "                pdf.ln(5)\n",
    "            \n",
    "            if 'components_fig' in report_data:\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", 'B', 14)\n",
    "                pdf.cell(0, 10, \"Forecast Components\", 0, 1)\n",
    "                pdf.set_font(\"Arial\", '', 11)\n",
    "                pdf.multi_cell(0, 5, \"This plot shows the forecast components: the overall trend, and the weekly and yearly seasonal patterns discovered in the data. This helps to understand the underlying drivers of the forecast.\")\n",
    "                pdf.ln(5)\n",
    "                model, forecast = report_data['components_fig']\n",
    "                fig = prophet_plot_components(model, forecast)\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmpfile:\n",
    "                    fig.savefig(tmpfile.name)\n",
    "                    pdf.image(tmpfile.name, w=180)\n",
    "                os.remove(tmpfile.name)\n",
    "                plt.close(fig)\n",
    "                pdf.ln(5)\n",
    "            \n",
    "            if 'segment_summary_bar' in report_data:\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", 'B', 14)\n",
    "                pdf.cell(0, 10, \"Forecast Summary by Segment\", 0, 1)\n",
    "                pdf.set_font(\"Arial\", '', 11)\n",
    "                pdf.multi_cell(0, 5, \"This bar chart compares the total forecasted value across different segments, providing a high-level overview of the predictions for each group.\")\n",
    "                pdf.ln(5)\n",
    "                summary_df = report_data['segment_summary_bar']\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                ax.bar(summary_df['Segment'], summary_df['Total Forecasted Value'])\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.tight_layout()\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmpfile:\n",
    "                    fig.savefig(tmpfile.name)\n",
    "                    pdf.image(tmpfile.name, w=160)\n",
    "                os.remove(tmpfile.name)\n",
    "                plt.close(fig)\n",
    "                pdf.ln(5)\n",
    "                \n",
    "            if 'segment_forecasts' in report_data and report_data['segment_forecasts']:\n",
    "                pdf.add_page()\n",
    "                pdf.set_font(\"Arial\", 'B', 14)\n",
    "                pdf.cell(0, 10, \"Detailed Segment Forecasts\", 0, 1)\n",
    "                for segment, (history, forecast) in report_data['segment_forecasts'].items():\n",
    "                    pdf.set_font(\"Arial\", 'B', 12)\n",
    "                    pdf.cell(0, 10, f\"Segment: {segment}\", 0, 1)\n",
    "                    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                    ax.plot(history['ds'].tail(365), history['y'].tail(365), label='Historical')\n",
    "                    ax.plot(forecast['ds'], forecast['yhat'], label='Forecast')\n",
    "                    ax.fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'], color='gray', alpha=0.2)\n",
    "                    ax.legend()\n",
    "                    plt.tight_layout()\n",
    "                    with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmpfile:\n",
    "                        fig.savefig(tmpfile.name)\n",
    "                        pdf.image(tmpfile.name, w=180)\n",
    "                    os.remove(tmpfile.name)\n",
    "                    plt.close(fig)\n",
    "                    pdf.ln(5)\n",
    "\n",
    "    return pdf.output(dest='S').encode('latin-1')\n",
    "\n",
    "def render_action_export_page():\n",
    "    # Conditionally skip this page for time-series models\n",
    "    if st.session_state.task == \"Predictive Modeling\" and st.session_state.user_selections.get('algorithm') in [\"Prophet\", \"ARIMA\"]:\n",
    "        return\n",
    "\n",
    "    st.header(\"Action & Export\")\n",
    "\n",
    "    if st.button(\"Go Back\"):\n",
    "        if st.session_state.task == \"Clustering Analysis\":\n",
    "            st.session_state.page = 'cluster_analysis'\n",
    "        else: # Predictive Modeling\n",
    "            st.session_state.page = 'evaluation'\n",
    "        st.rerun()\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    \n",
    "    if st.session_state.task == \"Clustering Analysis\":\n",
    "        st.subheader(\"Cluster Interpretation & Naming\")\n",
    "        df = st.session_state.processed_df\n",
    "        cluster_means = df.groupby('Cluster').mean()\n",
    "        population_means = df.drop(columns=['Cluster']).mean()\n",
    "\n",
    "        if not st.session_state.cluster_labels:\n",
    "            for cluster_id in df['Cluster'].unique():\n",
    "                st.session_state.cluster_labels[cluster_id] = f\"Cluster {cluster_id}\"\n",
    "        \n",
    "        col1, col2 = st.columns([1,2])\n",
    "        with col1:\n",
    "            st.write(\"#### Name Your Clusters\")\n",
    "            for cluster_id in sorted(df['Cluster'].unique()):\n",
    "                 st.session_state.cluster_labels[cluster_id] = st.text_input(\n",
    "                     f\"Label for Cluster {cluster_id}:\", \n",
    "                     value=st.session_state.cluster_labels[cluster_id]\n",
    "                 )\n",
    "        with col2:\n",
    "            st.write(\"#### Explore a Cluster\")\n",
    "            selected_cluster = st.selectbox(\"Select a cluster to profile:\", sorted(df['Cluster'].unique()))\n",
    "            \n",
    "            cluster_data = cluster_means.loc[selected_cluster]\n",
    "            comparison_df = pd.DataFrame({'Cluster Average': cluster_data, 'Population Average': population_means}).reset_index()\n",
    "            \n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Bar(x=comparison_df['index'], y=comparison_df['Cluster Average'], name='Cluster Average'))\n",
    "            fig.add_trace(go.Bar(x=comparison_df['index'], y=comparison_df['Population Average'], name='Population Average'))\n",
    "            fig.update_layout(barmode='group', title=f\"Profile of Cluster {selected_cluster}\", template='plotly_dark')\n",
    "            st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        st.markdown(\"---\")\n",
    "        st.subheader(\"Export Artifacts for Clustering\")\n",
    "        labeled_df = st.session_state.original_processed_df.copy()\n",
    "        labeled_df['Cluster ID'] = st.session_state.processed_df['Cluster']\n",
    "        labeled_df['Cluster Label'] = labeled_df['Cluster ID'].map(st.session_state.cluster_labels)\n",
    "        export_csv = labeled_df.to_csv().encode('utf-8')\n",
    "        st.download_button(\"Download Labeled Data (CSV)\", export_csv, \"labeled_cluster_data.csv\", \"text/csv\")\n",
    "        if st.session_state.model:\n",
    "            model_pkl = pickle.dumps(st.session_state.model)\n",
    "            st.download_button(\"Download Trained Model (PKL)\", model_pkl, \"model.pkl\")\n",
    "        \n",
    "        st.download_button(\"Download Full Report (PDF)\", create_pdf_report(st.session_state.get('report_data', {})), \"report.pdf\")\n",
    "\n",
    "\n",
    "    else: # Predictive Modeling\n",
    "        st.subheader(\"Live Prediction Simulator\")\n",
    "        if st.session_state.model and not (st.session_state.user_selections.get('algorithm') in [\"Prophet\", \"ARIMA\"]):\n",
    "            model = st.session_state.model\n",
    "            X_test = st.session_state.X_test\n",
    "            y_test = st.session_state.y_test\n",
    "            \n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                importances = model.feature_importances_\n",
    "            elif hasattr(model, 'coef_'):\n",
    "                importances = np.abs(model.coef_.flatten())\n",
    "            else:\n",
    "                importances = np.zeros(len(X_test.columns))\n",
    "\n",
    "            feature_importance_df = pd.DataFrame({'feature': X_test.columns, 'importance': importances}).sort_values('importance', ascending=False)\n",
    "            \n",
    "            base_features_df = feature_importance_df[~feature_importance_df['feature'].str.contains('_lag_|_roll_')]\n",
    "            top_features = base_features_df.head(5)['feature'].tolist()\n",
    "\n",
    "            input_data = {}\n",
    "            st.info(\"Adjust the top controllable features to see how they affect the prediction.\")\n",
    "            for feature in top_features:\n",
    "                if pd.api.types.is_numeric_dtype(X_test[feature].dtype):\n",
    "                    min_val, max_val = float(X_test[feature].min()), float(X_test[feature].max())\n",
    "                    input_data[feature] = st.slider(f\"Adjust {feature}\", min_val, max_val, float(X_test[feature].mean()))\n",
    "                else: \n",
    "                    unique_vals = list(X_test[feature].unique())\n",
    "                    input_data[feature] = st.selectbox(f\"Select {feature}\", options=unique_vals, index=0)\n",
    "\n",
    "            for col in X_test.columns:\n",
    "                if col not in input_data:\n",
    "                    input_data[col] = X_test[col].mean() if pd.api.types.is_numeric_dtype(X_test[col].dtype) else X_test[col].mode()[0]\n",
    "            \n",
    "            input_df = pd.DataFrame([input_data])[X_test.columns] \n",
    "            \n",
    "            prediction = model.predict(input_df)\n",
    "            pred_proba = model.predict_proba(input_df) if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "            if st.session_state.problem_type == \"Classification\":\n",
    "                st.success(f\"**Predicted Outcome:** `{prediction[0]}`\")\n",
    "                if pred_proba is not None:\n",
    "                    prob_df = pd.DataFrame(pred_proba, columns=model.classes_).T\n",
    "                    prob_df.columns = [\"Probability\"]\n",
    "                    st.bar_chart(prob_df)\n",
    "            else:\n",
    "                st.success(f\"**Predicted Value:** `{prediction[0]:,.2f}`\")\n",
    "                fig = go.Figure(go.Indicator(\n",
    "                    mode = \"gauge+number\",\n",
    "                    value = prediction[0],\n",
    "                    domain = {'x': [0, 1], 'y': [0, 1]},\n",
    "                    title = {'text': \"Predicted Value Gauge\"},\n",
    "                    gauge = {'axis': {'range': [y_test.min(), y_test.max()]}}\n",
    "                ))\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "# --- Main App Logic ---\n",
    "def main():\n",
    "    apply_custom_theme()\n",
    "    initialize_session_state()\n",
    "    render_sidebar()\n",
    "    page = st.session_state.page\n",
    "    \n",
    "    if page == 'upload': render_upload_page()\n",
    "    elif page == 'task_hub': render_task_hub()\n",
    "    elif page == 'preprocessing': render_preprocessing_page()\n",
    "    elif page == 'eda': render_eda_page()\n",
    "    elif page == 'action_export': render_action_export_page()\n",
    "    elif st.session_state.task == \"Predictive Modeling\":\n",
    "        if page == 'feature_engineering': render_feature_engineering_page()\n",
    "        elif page == 'model_recommendation': render_recommendation_page()\n",
    "        elif page == 'modeling': render_modeling_page()\n",
    "        elif page == 'evaluation': render_evaluation_page()\n",
    "    elif st.session_state.task == \"Clustering Analysis\":\n",
    "        if page == 'clustering': render_clustering_page()\n",
    "        elif page == 'cluster_analysis': render_cluster_analysis_page()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab6afc1-b828-4c5a-a8e7-03cb6040c98a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
